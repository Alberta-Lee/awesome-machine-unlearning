{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tamlhp/machine-unlearning-the-right-to-be-forgotten?scriptVersionId=135859676\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b7f3e3dd","metadata":{"papermill":{"duration":0.004597,"end_time":"2023-07-05T23:39:33.375662","exception":false,"start_time":"2023-07-05T23:39:33.371065","status":"completed"},"tags":[]},"source":["# Machine Unlearning: The Right to be Forgotten\n","\n","## Abstract\n","\n","Today, computer systems hold large amounts of personal data. Yet while\n","such an abundance of data allows breakthroughs in artificial\n","intelligence, and especially machine learning, its existence can be a\n","threat to user privacy, and it can weaken the bonds of trust between\n","humans and AI. Recent regulations now require that, on request, private\n","information about a user must be removed from both computer systems and\n","from machine learning models -- this legislation is more colloquially\n","called \"the right to be forgotten\"). While removing data from back-end\n","databases should be straightforward, it is not sufficient in the AI\n","context as machine learning models often 'remember' the old data.\n","Contemporary adversarial attacks on trained models have proven that we\n","can learn whether an instance or an attribute belonged to the training\n","data. This phenomenon calls for a new paradigm, namely *machine\n","unlearning*, to make machine learning models forget about particular\n","data. It turns out that recent works on machine unlearning have not been\n","able to completely solve the problem due to the lack of common\n","frameworks and resources. Therefore, this paper aspires to present a\n","comprehensive examination of machine unlearning's concepts, scenarios,\n","methods, and applications. Specifically, as a category collection of\n","cutting-edge studies, the intention behind this article is to serve as a\n","comprehensive resource for researchers and practitioners seeking an\n","introduction to machine unlearning and its formulations, design\n","criteria, removal requests, algorithms, and applications. In addition,\n","we aim to highlight the key findings, current trends, and new research\n","areas that have not yet featured the use of machine unlearning but could\n","benefit greatly from it. We hope this survey serves as a valuable\n","resource for machine learning researchers and those seeking to innovate\n","privacy technologies. Our resources are publicly available at\n","<https://github.com/tamlhp/awesome-machine-unlearning>.\n","\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/framework.png\" alt=\"image\" style=\"max-width: 100%;\"/>\n","<figcaption aria-hidden=\"true\">A Typical Machine Unlearning Process</figcaption>\n","</figure>\n","</div>"]},{"cell_type":"markdown","id":"b79117e0","metadata":{"papermill":{"duration":0.004587,"end_time":"2023-07-05T23:39:33.384111","exception":false,"start_time":"2023-07-05T23:39:33.379524","status":"completed"},"tags":[]},"source":["<h1 id=\"sec:intro\">1. Introduction</h1>\n","<p>Computer systems today hold large amounts of personal data. Due to\n","the great advancement in data storage and data transfer technologies,\n","the amount of data being produced, recorded, and processed has exploded.\n","For example, four billion YouTube videos are watched every day&#xA0;<a href=\"#ref-sari2020learning\">(Sari et al.\n","2020)</a>. These online personal data, including digital footprints\n","made by (or about) netizens, reflects their behaviors, interactions, and\n","communication patterns in real-world&#xA0;<a href=\"#ref-nguyen2019debunking\">(Thanh Tam Nguyen 2019)</a>. Other\n","sources of personal data include the digital content that online users\n","create to express their ideas and opinions, such as product reviews,\n","blog posts (e.g.&#xA0;Medium), status seeking (e.g.&#xA0;Instagram), and knowledge\n","sharing (e.g. Wikipedia)&#xA0;<a href=\"#ref-nguyen2021judo\">(Thanh Toan Nguyen et al. 2021)</a>. More\n","recently, personal data has also expanded to include data from wearable\n","devices&#xA0;<a href=\"#ref-ren2022prototype\">(Z. Ren,\n","Nguyen, and Nejdl 2022)</a>. On the one hand, such an abundance of\n","data has helped to advance artificial intelligence (AI). However, on the\n","other hand, it threatens the privacy of users and has led to many data\n","breaches&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and\n","Yang 2015)</a>. For this reason, some users may choose to have their\n","data completely removed from a system, especially sensitive systems such\n","as those do with finance or healthcare&#xA0;<a href=\"#ref-ren2022prototype\">(Z. Ren, Nguyen, and Nejdl 2022)</a>.\n","Recent regulations now compel organisations to give users &#x201C;the right to\n","be forgotten&#x201D;, i.e., the right to have all or part of their data deleted\n","from a system on request&#xA0;<a href=\"#ref-dang2021right\">(Dang 2021)</a>.</p>\n","<p>While removing data from back-end databases satisfies the\n","regulations, doing so is not sufficient in the AI context as machine\n","learning models often &#x2018;remember&#x2019; the old data. Indeed, in machine\n","learning systems, often millions, if not billions, of users&#x2019; data have\n","been processed during the model&#x2019;s training phase. However, unlike humans\n","who learn general patterns, machine learning models behave more like a\n","lossy data compression mechanism&#xA0;<a href=\"#ref-schelter2020amnesia\">(Schelter 2020)</a>, and some are\n","overfit against their training data. The success of deep learning models\n","in particular has been recently been attributed to the compression of\n","training data&#xA0;<a href=\"#ref-tishby2000information tishby2015deep\">(Tishby et al. 2000;\n","Tishby and Zaslavsky 2015)</a>. This memorization behaviour can be\n","further proven by existing works on adversarial attacks&#xA0;<a href=\"#ref-ren2020generating chang2022example ren2020enhancing\">(Z.\n","Ren, Baird, et al. 2020; Chang et al. 2022; Z. Ren, Han, et al.\n","2020)</a>, which have shown that it is possible to extract the\n","private information within some target data from a trained model.\n","However, we also know that the parameters of a trained model do not tend\n","to show any clear connection to the data that was used for\n","training&#xA0;<a href=\"#ref-shwartz2017opening\">(Shwartz-Ziv and Tishby 2017)</a>. As\n","a result, it can be challenging to remove information corresponding to a\n","particular data item from a machine learning model. In other words, it\n","can be difficult to make a machine learning model forget a user&#x2019;s\n","data.</p>\n","<div class=\"table*\">\n","\n","</div>\n","<p>This challenge of allowing users the possibility and flexibility to\n","completely delete their data from a machine learning model calls for a\n","new paradigm, namely <em>machine unlearning</em>&#xA0;<a href=\"#ref-nguyen2022markov baumhauer2020machine tahiliani2021machine\">(Q.\n","P. Nguyen et al. 2022; Baumhauer, Sch&#xF6;ttle, and Zeppelzauer 2020;\n","Tahiliani et al. 2021)</a>. Ideally, a machine unlearning mechanism\n","would remove data from the model without needing to retrain it from\n","scratch&#xA0;<a href=\"#ref-nguyen2022markov\">(Q. P.\n","Nguyen et al. 2022)</a>. To this end, a users&#x2019; right to be forgotten\n","would be observed and the model owner would be shielded from constant\n","and expensive retraining exercises.</p>\n","<p>Researchers have already begun to study aspects of machine\n","unlearning, such as removing part of the training data and analysing the\n","subsequent model predictions&#xA0;<a href=\"#ref-nguyen2022markov thudi2021necessity\">(Q. P. Nguyen et al.\n","2022; Thudi, Jia, et al. 2022)</a>. However, it turns out that this\n","problem cannot be completely solved due to a lack of common frameworks\n","and resources&#xA0;<a href=\"#ref-villaronga2018humans veale2018algorithms shintre2019making schelter2020amnesia\">(Villaronga,\n","Kieseberg, and Li 2018; Veale, Binns, and Edwards 2018; Shintre et al.\n","2019; Schelter 2020)</a>. Hence, to begin building a foundation of\n","works in this nascent area, we undertook a comprehensive survey of\n","machine unlearning: its definitions, scenarios, mechanisms, and\n","applications. Our resources are publicly available at&#xA0;<a href=\"#fn1\"\n","class=\"footnote-ref\" id=\"fnref1\"\n","role=\"doc-noteref\"><sup>1</sup></a>.</p>\n","<h2 id=\"reasons-for-machine-unlearning\">Reasons for Machine\n","Unlearning</h2>\n","<p>There are many reasons for why a users may want to delete their data\n","from a system. We have categorized these into four major groups:\n","security, privacy, usability, and fidelity. Each reason is discussed in\n","more detail next.</p>\n","<p><strong>Security.</strong> Recently, deep learning models have been\n","shown to be vulnerable to external attacks, especially adversarial\n","attacks&#xA0;<a href=\"#ref-ren2020adversarial\">(K. Ren\n","et al. 2020)</a>. In an adversarial attack, the attacker generates\n","adversarial data that are very similar to the original data to the\n","extent that a human cannot distinguish between the real and fake data.\n","This adversarial data is designed to force the deep learning models into\n","outputting wrong predictions, which frequently results in serious\n","problems. For example, in healthcare, a wrong prediction could lead to a\n","wrong diagnosis, a non-suitable treatment, even a death. Hence,\n","detecting and removing adversarial data is essential for ensuring the\n","model&#x2019;s security and, once an attack is detected, the model needs to be\n","able delete the adversarial data through a machine unlearning\n","mechanism&#xA0;<a href=\"#ref-cao2015towards marchant2022hard\">(Y. Cao and Yang 2015;\n","Marchant, Rubinstein, and Alfeld 2022)</a>.</p>\n","<p><strong>Privacy.</strong> Many privacy-preserving regulations have\n","been enacted recently that involve the right to be forgotten&#x201D;&#xA0;<a href=\"#ref-bourtoule2021machine dang2021right\">(Bourtoule et al. 2021;\n","Dang 2021)</a>, such as the European Union&#x2019;s General Data Protection\n","Regulation (GDPR)&#xA0;<a href=\"#ref-magdziarczyk2019right\">(Mantelero 2013)</a> and the\n","California Consumer Privacy Act&#xA0;<a href=\"#ref-pardau2018california\">(Pardau 2018)</a>. In this\n","particular regulation, users must be given the right to have their data\n","and related information deleted to protect their privacy. In part, this\n","legislation has sprung up as a result of privacy leaks. For example,\n","cloud systems can leak user data due to multiple copies of data hold by\n","different parties, backup policies, and replication strategies&#xA0;<a href=\"#ref-singh2017data\">(A. Singh and Anand\n","2017)</a>. In another case, machine learning approaches for genetic\n","data processing were found to leak patients&#x2019; genetic markers&#xA0;<a href=\"#ref-fredrikson2014privacy wang2009learning\">(Fredrikson et al.\n","2014; R. Wang et al. 2009)</a>. It is therefore not surprising that\n","users would want to remove their data to avoid the risks of a data\n","leak&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang\n","2015)</a>.</p>\n","<p><strong>Usability.</strong> People have difference preferences in\n","online applications and/or services, especially recommender systems. An\n","application will produce inconvenient recommendations if it cannot\n","completely delete the incorrect data (e.&#x2006;g.,&#xA0;noise, malicious data,\n","out-of-distribution data) related to a user. For example, one can\n","accidentally search for an illegal product on his laptop, and find that\n","he keeps getting this product recommendation on this phone, even after\n","he cleared his web browser history&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>. Such\n","undesired usability by not forgetting data will not only produce wrong\n","predictions, but also result in less users.</p>\n","<p><strong>Fidelity.</strong> Unlearning requests might come from biased\n","machine learning models. Despite recent advances, machine learning\n","models are still sensitive to bias that means their output can unfairly\n","discriminate against a group of people&#xA0;<a href=\"#ref-mehrabi2021survey\">(Mehrabi et al. 2021)</a>. For\n","example, COMPAS, the software used by courts to decide parole cases, is\n","more likely to consider African-American offenders to have higher risk\n","scores than Caucasians, even though ethnicity information is not part of\n","the input&#xA0;<a href=\"#ref-zou2018ai\">(Zou and\n","Schiebinger 2018)</a>. Similar situations have been observed in\n","beauty contest judged by AI, which was biased against contestants with\n","darker skin tones, or facial recognition AI that wrongly recognized\n","Asian facial features&#xA0;<a href=\"#ref-feuerriegel2020fair\">(Feuerriegel, Dolata, and Schwabe\n","2020)</a>.</p>\n","<p>The source of these biases often originate from data. For example, AI\n","systems that have been trained on public datasets that contain mostly\n","white persons, such as ImageNet, are likely to make errors when\n","processing images of black persons. Similarly, in an application\n","screening system, inappropriate features, such as the gender or race of\n","applicants, might be unintentionally learned by the machine learning\n","model&#xA0;<a href=\"#ref-dinsdale2021deep dinsdale2020unlearning\">(Dinsdale,\n","Jenkinson, and Namburete 2021; Dinsdale, Jenkinson, et al. 2020)</a>.\n","As a result, there is a need to unlearn these data, including the\n","features and affected data items.</p>\n","<h2 id=\"challenges-in-machine-unlearning\">Challenges in Machine\n","Unlearning</h2>\n","<p>Before we can truly achieve machine unlearning, several challenges to\n","removing specific parts of the training data need to be overcome. The\n","challenges are summarized as follows.</p>\n","<p><strong>Stochasticity of training.</strong> We do not know the impact\n","of each data point seen during training on the machine learning model\n","due to the stochastic nature of the training procedure&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. Neural networks, for example, are usually trained on\n","random mini-batches containing a certain number of data samples.\n","Further, the order of the training batches is also random&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. This stochasticity raises difficulties for machine\n","unlearning as the specific data sample to be removed would need to be\n","removed from all batches.</p>\n","<p><strong>Incrementality of training.</strong> A model&#x2019;s training\n","procedure is an incremental process&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>. In\n","other words, the model update on a given data sample will affect the\n","model performance on data samples fed into the model after this data. A\n","model&#x2019;s performance on this given data sample is also affected by prior\n","data samples. Determining a way to erase the effect of the to-be-removed\n","training sample on further model performance is a challenge for machine\n","unlearning.</p>\n","<p><strong>Catastrophic unlearning.</strong> In general, an unlearned\n","model usually performs worse than the model retrained on the remaining\n","data&#xA0;<a href=\"#ref-nguyen2020variational nguyen2022markov\">(Q. P. Nguyen, Low,\n","and Jaillet 2020; Q. P. Nguyen et al. 2022)</a>. However, the\n","degradation can be exponential when more data is unlearned. Such sudden\n","degradation is often referred as catastrophic unlearning&#xA0;<a href=\"#ref-nguyen2020variational\">(Q. P. Nguyen, Low,\n","and Jaillet 2020)</a>. While several studies&#xA0;<a href=\"#ref-du2019lifelong golatkar2020eternal\">(Du, Chen, et al. 2019;\n","Golatkar, Achille, and Soatto 2020a)</a> have explored ways to\n","mitigate catastrophic unlearning by designing special loss functions,\n","how to naturally prevent catastrophic unlearning is still an open\n","question.</p>\n","<h2 id=\"contributions-of-this-survey\">Contributions of this survey</h2>\n","<p>The aim of this survey is to supply a complete examination of research\n","studies on machine unlearning as well as a discussion on potential new\n","research directions in machine unlearning. The contributions of our\n","survey can therefore be summarized as follows.</p>\n","<div class=\"compactitem\">\n","<p>First, we show how to design an unlearning framework. We discuss the\n","design requirements, different types of unlearning requests, and how to\n","verify the unlearned model. The details can be found in <a\n","href=\"#sec:framework\" data-reference-type=\"autoref\"\n","data-reference=\"sec:framework\">[sec:framework]</a>.</p>\n","<p>Second, we show how to define an unlearning problem in machine\n","learning systems. This includes the formulation of exact unlearning and\n","approximate unlearning as well as the definition of indistinguishability\n","metrics to compare two given models (i.e., the unlearned model and the\n","retrained model). The details are discussed in <a href=\"#sec:problem\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:problem\">[sec:problem]</a>.</p>\n","<p>Third, we discuss different scenarios of machine unlearning,\n","including zero-glance unlearning, zero-shot unlearning, and few-shot\n","unlearning. The details are provided in <a href=\"#sec:scenarios\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:scenarios\">[sec:scenarios]</a></p>\n","<p>Fourth, we introduce a unified taxonomy that categorizes the machine\n","unlearning approaches into three branches: model-agnostic methods,\n","model-intrinsic methods, and data-driven methods. The details can be\n","found in <a href=\"#sec:algorithms\" data-reference-type=\"autoref\"\n","data-reference=\"sec:algorithms\">[sec:algorithms]</a>.</p>\n","<p>Fifth, we compile a variety of regularly used datasets and\n","open-source implementations to serve as a foundation for future machine\n","unlearning research and benchmarking. The details are provided in <a\n","href=\"#sec:resources\" data-reference-type=\"autoref\"\n","data-reference=\"sec:resources\">[sec:resources]</a>.</p>\n","<p>Finally, we highlight the findings, trends and the forthcoming\n","according to our survey results in <a href=\"#sec:discussion\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:discussion\">[sec:discussion]</a>. <a\n","href=\"#sec:conclusion\" data-reference-type=\"autoref\"\n","data-reference=\"sec:conclusion\">[sec:conclusion]</a> then completes the\n","paper.</p>\n","</div>\n","<h2 id=\"differences-between-this-and-previous-surveys\">Differences\n","between this and previous surveys</h2>\n","<p><a href=\"#tab:survey_comparison\" data-reference-type=\"autoref\"\n","data-reference=\"tab:survey_comparison\">[tab:survey_comparison]</a>\n","summarizes the differences between our survey and existing efforts to\n","unify the field. It is noteworthy that machine unlearning is different\n","from data deletion&#xA0;<a href=\"#ref-garg2020formalizing\">(Garg, Goldwasser, and Vasudevan\n","2020)</a>. Both topics concern the right to be forgotten legislated\n","and exercised across the world&#xA0;<a href=\"#ref-magdziarczyk2019right\">(Mantelero 2013)</a>. However, the\n","latter focuses only on the data perspective following the General Data\n","Protection Regulation (GDPR)&#xA0;<a href=\"#ref-voigt2017eu\">(Voigt and Von dem Bussche 2017)</a>, while\n","machine unlearning also addresses privacy problems from a model\n","perspective.</p>\n","<p>There are some other concepts that might be mistaken as machine\n","unlearning, such as data redaction that aims to poison the label\n","information of the data to be forgotten inside the model&#xA0;<a href=\"#ref-felps2020class\">(Felps et al. 2020)</a>.\n","In other words, it forces the model make wrong predictions about the\n","forgotten data. Although applicable in some setting, this approach is\n","not fully compatible with machine unlearning as the forgotten data has\n","to be known a priori when the original model is trained&#xA0;<a href=\"#ref-felps2020class\">(Felps et al.\n","2020)</a>.</p>\n","<h1 id=\"sec:framework\">Unlearning Framework</h1>\n","<h2 id=\"unlearning-workflow\">Unlearning Workflow</h2>\n","<p>The unlearning framework in <a href=\"#fig:unlearning_workflow\"\n","data-reference-type=\"autoref\"\n","data-reference=\"fig:unlearning_workflow\">[fig:unlearning_workflow]</a>\n","presents the typical workflow of a machine learning model in the\n","presence of a data removal request. In general, a model is trained on\n","some data and is then used for inference. Upon a removal request, the\n","data-to-be-forgotten is unlearned from the model. The unlearned model is\n","then verified against privacy criteria, and, if these criteria are not\n","met, the model is retrained, i.e., if the model still leaks some\n","information about the forgotten data. There are two main components to\n","this process: the <em>learning component</em> (left) and the\n","<em>unlearning component</em> (right). The learning component involves\n","the current data, a learning algorithm, and the current model. In the\n","beginning, the initial model is trained from the whole dataset using the\n","learning algorithm. The unlearning component involves an unlearning\n","algorithm, the unlearned model, optimization requirements, evaluation\n","metrics, and a verification mechanism. Upon a data removal request, the\n","current model will be processed by an unlearning algorithm to forget the\n","corresponding information of that data inside the model. The unlearning\n","algorithm might take several requirements into account such as\n","completeness, timeliness, and privacy guarantees. The outcome is an\n","unlearned model, which will be evaluated against different performance\n","metrics (e.g., accuracy, ZRF score, anamnesis index). However, to\n","provide a privacy certificate for the unlearned model, a verification\n","(or audit) is needed to prove that the model actually forgot the\n","requested data and that there are no information leaks. This audit might\n","include a feature injection test, a membership inference attack,\n","forgetting measurements, etc.</p>\n","<div class=\"figure*\">\n","<figure>\n","<img src=\"https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/framework.png\" alt=\"image\" style=\"max-width: 100%;\"/>\n","<figcaption aria-hidden=\"true\">A Typical Machine Unlearning Process</figcaption>\n","</figure>\n","</div>\n","<p>If the unlearned model passes the verification, it becomes the new\n","model for downstream tasks (e.g., inference, prediction, classification,\n","recommendation). If the model does not pass verification, the remaining\n","data, i.e., the original data excluding the data to be forgotten, needs\n","to be used to retrain the model. Either way, the unlearning component\n","will be called repeatedly upon a new removal request.</p>\n","<h2 id=\"unlearning-requests\">Unlearning Requests</h2>\n","<p><strong>Item Removal.</strong> Requests to remove certain\n","items/samples from the training data are the most common requests in\n","machine unlearning&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>. The\n","techniques used to unlearn these data are described in detail in <a\n","href=\"#sec:algorithms\" data-reference-type=\"autoref\"\n","data-reference=\"sec:algorithms\">[sec:algorithms]</a>.</p>\n","<p><strong>Feature Removal.</strong> In many scenarios, privacy leaks\n","might not only originate from a single data item but also in a group of\n","data with the similar features or labels&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>. For\n","example, a poisoned spam filter might misclassify malicious addresses\n","that are present in thousands of emails. Thus, unlearning suspicious\n","emails might not enough. Similarly, in an application screening system,\n","inappropriate features, such as the gender or race of applicants, might\n","need to be unlearned for thousands of affected applications.</p>\n","<p>In such cases, naively unlearning the affected data items\n","sequentially is imprudent as repeated retraining is computationally\n","expensive. Moreover, unlearning too many data items can inherently\n","reduce the performance of the model, regardless of the unlearning\n","mechanism used. Thus, there is a need for unlearning data at the feature\n","or label level with an arbitrary number of data items.</p>\n","<p>Warnecke et al.&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a> proposed\n","a technique for unlearning a group of training data based on influence\n","functions. More precisely, the effect of training data on model\n","parameter updates is estimated and formularized in closed-form. As a\n","result of this formulation, influences of the learning sets act as a\n","compact update instead of solving an optimisation problem iteratively\n","(e.g., loss minimization). First-order and second-order derivatives are\n","the keys to computing this update effectively&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>.</p>\n","<p>Guo et al.&#xA0;<a href=\"#ref-guo2022efficient\">(T.\n","Guo et al. 2022)</a> proposed another technique to unlearn a feature\n","in the data based on disentangled representation. The core idea is to\n","learn the correlation between features from the latent space as well as\n","the effects of each feature on the output space. Using this information,\n","certain features can be progressively detached from the learnt model\n","upon request, while the remaining features are still preserved to\n","maintain good accuracy. However, this method is mostly applicable to\n","deep neural networks in the image domain, in which the deeper\n","convolutional layers become smaller and can therefore identify abstract\n","features that match real-world data attributes.</p>\n","<p><strong>Class Removal.</strong> There are many scenarios where the\n","forgetting data belongs to single or multiple classes from a trained\n","model. For example, in face recognition applications, each class is a\n","person&#x2019;s face so there could potentially be thousands or millions of\n","classes. However, when a user opts out of the system, their face\n","information must be removed without using a sample of their face.</p>\n","<p>Similar to feature removal, class removal is more challenging than\n","item removal because retraining solutions can incur many unlearning\n","passes. Even though each pass might only come at a small computational\n","cost due to data partitioning, the expense mounts up. However,\n","partitioning data by class itself does not help the model&#x2019;s training in\n","the first place, as learning the differences between classes is the core\n","of many learning algorithms&#xA0;<a href=\"#ref-tanha2020boosting\">(Tanha et al. 2020)</a>. Although some\n","of the above techniques for feature removal can be applied to class\n","removal&#xA0;<a href=\"#ref-warnecke2021machine\">(Warnecke et al. 2021)</a>, it is\n","not always the case as class information might be implicit in many\n","scenarios.</p>\n","<p>Tarun et al.&#xA0;<a href=\"#ref-tarun2021fast\">(Tarun\n","et al. 2021)</a> proposed an unlearning method for class removal\n","based on data augmentation. The basic concept is to introduce noise into\n","the model such that the classification error is maximized for the target\n","class(es). The model is updated by training on this noise without the\n","need to access any samples of the target class(es). Since such impair\n","step may disturb the model weights and degrade the classification\n","performance for the remaining classes, a repair step is needed to train\n","the model for one or a few more epochs on the remaining data. Their\n","experiments show that the method can be efficient for large-scale\n","multi-class problems (100 classes). Further, the method worked\n","especially well with face recognition tasks because the deep neural\n","networks were originally trained on triplet loss and negative samples so\n","the difference between the classes was quite significant&#xA0;<a href=\"#ref-masi2018deep\">(Masi et al.\n","2018)</a>.</p>\n","<p>Baumhauer et al.&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a> proposed an unlearning method for class removal based on a\n","linear filtration operator that proportionally shifts the classification\n","of the samples of the class to be forgotten to other classes. However,\n","the approach is only applicable to class removal due to the\n","characteristics of this operator.</p>\n","<p><strong>Task Removal.</strong> Today, machine learning models are not\n","only trained for a single task but also for multiple tasks. This\n","paradigm, aka continual learning or lifelong learning&#xA0;<a href=\"#ref-parisi2019continual\">(Parisi et al.\n","2019)</a>, is motivated by the human brain, in which learning\n","multiple tasks can benefit each other due to their correlations. This\n","technique is also used overcome data sparsity or cold-start problems\n","where there is not enough data to train a single task effectively.</p>\n","<p>However, in these settings too, there can be a need to remove private\n","data related to a specific task. For example, consider a robot that is\n","trained to assist a patient at home during their medical treatment. This\n","robot may be asked to forget this assistance behaviour after the patient\n","has recovered&#xA0;<a href=\"#ref-liu2022continual\">(B.\n","Liu, Liu, et al. 2022)</a>. To this end, temporarily learning a task\n","and forgetting it in the future has become a need for lifelong learning\n","models.</p>\n","<p>In general, unlearning a task is uniquely challenging as continual\n","learning might depend on the order of the learned tasks. Therefore,\n","removing a task might create a catastrophic unlearning effect, where the\n","overall performance of multiple tasks is degraded in a\n","domino-effect&#xA0;<a href=\"#ref-liu2022continual\">(B.\n","Liu, Liu, et al. 2022)</a>. Mitigating this problem requires the\n","model to be aware of that the task may potentially be removed in future.\n","Liu et al.&#xA0;<a href=\"#ref-liu2022continual\">(B. Liu,\n","Liu, et al. 2022)</a> explains that this requires users to explicitly\n","define which tasks will be learned permanently and which tasks will be\n","learned only temporarily.</p>\n","<p><strong>Stream Removal.</strong> Handling data streams where a huge\n","amount of data arrives online requires some mechanisms to retain or\n","ignore certain data while maintaining limited storage&#xA0;<a href=\"#ref-nguyen2017retaining\">(Tam et al.\n","2017)</a>. In the context of machine unlearning, however, handling\n","data streams is more about dealing with a stream of removal\n","requests.</p>\n","<p>Gupta et el.&#xA0;<a href=\"#ref-gupta2021adaptive\">(Gupta et al. 2021)</a> proposed a\n","streaming unlearning setting involving a sequence of data removal\n","requests. This is motivated by the fact that many users can be involved\n","in a machine learning system and decide to delete their data\n","sequentially. Such is also the case when the training data has been\n","poisoned in an adversarial attack and the data needs to be deleted\n","gradually to recover the model&#x2019;s performance. These streaming requests\n","can be either non-adaptive or adaptive. A non-adaptive request means\n","that the removal sequence does not depend on the intermediate results of\n","each unlearning request, whereas and adaptive request means that the\n","data to be removed depends on the current unlearned model. In other\n","words, after the poisonous data is detected, the model is unlearned\n","gradually so as to decide which data item is most beneficial to unlearn\n","next.</p>\n","<h2 id=\"design-requirements\">Design Requirements</h2>\n","<p><strong>Completeness (Consistency).</strong> A good unlearning\n","algorithm should be complete&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>, i.e.&#xA0;the\n","unlearned model and the retrained model make the same predictions about\n","any possible data sample (whether right or wrong). One way to measure\n","this consistency is to compute the percentage of the same prediction\n","results on a test data. This requirement can be designed as an\n","optimization objective in an unlearning definition (<a\n","href=\"#sec:exact_unlearning\" data-reference-type=\"autoref\"\n","data-reference=\"sec:exact_unlearning\">[sec:exact_unlearning]</a>) by\n","formulating the difference between the output space of the two models.\n","Many works on adversarial attacks can help with this formulation&#xA0;<a href=\"#ref-sommer2022athena chen2021machine\">(Sommer\n","et al. 2022; M. Chen et al. 2021b)</a>.</p>\n","<p><strong>Timeliness.</strong> In general, retraining can fully solve\n","any unlearning problem. However, retraining is time-consuming,\n","especially when the distribution of the data to be forgotten is\n","unknown&#xA0;<a href=\"#ref-cao2015towards bourtoule2021machine\">(Y. Cao and Yang 2015;\n","Bourtoule et al. 2021)</a>. As a result, there needs to be a\n","trade-off between completeness and timeliness. Unlearning techniques\n","that do not use retraining might be inherently not complete, i.e., they\n","may lead to some privacy leaks, even though some provable guarantees are\n","provided for special cases&#xA0;<a href=\"#ref-GuoGHM20 marchant2022hard neel2021descent\">(C. Guo et al.\n","2020; Marchant, Rubinstein, and Alfeld 2022; Neel, Roth, and\n","Sharifi-Malvajerdi 2021)</a>. To measure timeliness, we can measure\n","the speed up of unlearning over retraining after an unlearning request\n","is invoked.</p>\n","<p>It is also worth recognizing the cause of this trade-off between\n","retraining and unlearning. When there is not much data to be forgotten,\n","unlearning is generally more beneficial as the effects on model accuracy\n","are small. However, when there is much forgetting data, retraining might\n","be better as unlearning many times, even bounded, may catastrophically\n","degrade the model&#x2019;s accuracy&#xA0;<a href=\"#ref-cao2015towards\">(Y. Cao and Yang 2015)</a>.</p>\n","<p><strong>Accuracy.</strong> An unlearned model should be able to\n","predict test samples correctly. Or at least its accuracy should be\n","comparable to the retrained model. However, as retraining is\n","computationally costly, retrained models are not always available for\n","comparison. To address this issue, the accuracy of the unlearned model\n","is often measured on a new test set, or it is compared with that of the\n","original model before unlearning&#xA0;<a href=\"#ref-he2021deepobliviate\">(He et al. 2021)</a>.</p>\n","<p><strong>Light-weight.</strong> To prepare for unlearning process,\n","many techniques need to store model checkpoints, historical model\n","updates, training data, and other temporary data&#xA0;<a href=\"#ref-he2021deepobliviate bourtoule2021machine liu2020federated\">(He\n","et al. 2021; Bourtoule et al. 2021; G. Liu et al. 2020)</a>. A good\n","unlearning algorithm should be light-weight and scale with big data. Any\n","other computational overhead beside unlearning time and storage cost\n","should be reduced as well&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>.</p>\n","<p><strong>Provable guarantees.</strong> With the exception of\n","retraining, any unlearning process might be inherently approximate. It\n","is practical for an unlearning method to provide a provable guarantee on\n","the unlearned model. To this end, many works have designed unlearning\n","techniques with bounded approximations on retraining&#xA0;<a href=\"#ref-GuoGHM20 marchant2022hard neel2021descent\">(C. Guo et al.\n","2020; Marchant, Rubinstein, and Alfeld 2022; Neel, Roth, and\n","Sharifi-Malvajerdi 2021)</a>. Nonetheless, these approaches are\n","founded on the premise that models with comparable parameters will have\n","comparable accuracy.</p>\n","<p><strong>Model-agnostic.</strong> An unlearning process should be\n","generic for different learning algorithms and machine learning\n","models&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>,\n","especially with provable guarantees as well. However, as machine\n","learning models are different and have different learning algorithms as\n","well, designing a model-agnostic unlearning framework could be\n","challenging.</p>\n","<p><strong>Verifiability.</strong> Beyond unlearning requests, another\n","demand by users is to verify that the unlearned model now protects their\n","privacy. To this end, a good unlearning framework should provide\n","end-users with a verification mechanism. For example, backdoor attacks\n","can be used to verify unlearning by injecting backdoor samples into the\n","training data&#xA0;<a href=\"#ref-sommer2020towards\">(Sommer et al. 2020)</a>. If the\n","backdoor can be detected in the original model while not detected in the\n","unlearned model, then verification is considered to be a success.\n","However, such verification might be too intrusive for a trustworthy\n","machine learning system and the verification might still introduce false\n","positive due to the inherent uncertainty in backdoor detection.</p>\n","<h2 id=\"unlearning-verification\">Unlearning Verification</h2>\n","<p>The goal of unlearning verification methods is to certify that one\n","cannot easily distinguish between the unlearned models and their\n","retrained counterparts&#xA0;<a href=\"#ref-thudi2021necessity\">(Thudi, Jia, et al. 2022)</a>. While\n","the evaluation metrics (<a href=\"#sec:metrics\"\n","data-reference-type=\"autoref\"\n","data-reference=\"sec:metrics\">[sec:metrics]</a>) are theoretical criteria\n","for machine unlearning, unlearning verification can act as a certificate\n","for an unlearned model. They also include best practices for validating\n","the unlearned models efficiently.</p>\n","<p>It is noteworthy that while unlearning metrics (in <a\n","href=\"#sec:formulation\" data-reference-type=\"autoref\"\n","data-reference=\"sec:formulation\">[sec:formulation]</a>) and verification\n","metrics share some overlaps, the big difference is that the former can\n","be used for optimization or to provide a bounded guarantee, while the\n","latter is used for evaluation only.</p>\n","<p><strong>Feature Injection Test.</strong> The goal of this test is to\n","verify whether the unlearned model has adjusted the weights\n","corresponding to the removed data samples based on data\n","features/attributes&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al. 2021)</a>. The idea is\n","that if the set of data to be forgotten has a very distinct feature\n","distinguishing it from the remaining set, it gives a strong signal for\n","the model weights. However, this feature needs to be correlated with the\n","labels of the set to be forgotten, otherwise the model might not learn\n","anything from this feature.</p>\n","<p>More precisely, an extra feature is added for each data item such\n","that it is equal to zero for the remaining set and is perfectly\n","correlated with the labels of the set to forget. Izzo et al.&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al.\n","2021)</a> applied this idea with linear classifiers, where the weight\n","associated with this extra feature is expected to be significantly\n","different from zero after training. After the model is unlearned, this\n","weight is expected to become zero. As a result, the difference of this\n","weight can be plotted before and after unlearning as a measure of\n","effectiveness of the unlearning process.</p>\n","<p>One limitation of this verification method is that the current\n","solution is only applicable for linear and logistic models&#xA0;<a href=\"#ref-izzo2021approximate\">(Izzo et al.\n","2021)</a>. This is because these models have explicit weights\n","associated with the injected feature, whereas, for other models such as\n","deep learning, injecting such a feature as a strong signal is\n","non-trivial, even though the set to be forgotten is small. Another\n","limitation to these types of methods is that an injected version of the\n","data needs to be created so that the model can be learned (either from\n","scratch or incrementally depending on the type of the model).</p>\n","<p><strong>Forgetting Measuring.</strong> Even after the data to be\n","forgotten has been unlearned from the model, it is still possible for\n","the model to carry detectable traces of those samples&#xA0;<a href=\"#ref-jagielski2022measuring\">(Jagielski et al.\n","2022)</a>. Jagielski et al.&#xA0;<a href=\"#ref-jagielski2022measuring\">(Jagielski et al. 2022)</a>\n","proposed a formal way to measure the forgetfulness of a model via\n","privacy attacks. More precisely, a model is said to <span\n","class=\"math inline\"><em>&#x3B1;</em></span>-forget a training sample if a\n","privacy attack (e.g., a membership inference) on that sample achieves no\n","greater than success rate <span class=\"math inline\"><em>&#x3B1;</em></span>.\n","This definition is more flexible than differential privacy because a\n","training algorithm is differentially private only if it immediately\n","forgets every sample it learns. As a result, this definition allows a\n","sample to be temporarily learned, and measures how long until it is\n","forgotten by the model.</p>\n","<p><strong>Information Leakage.</strong> Many machine learning models\n","inherently leak information during the model updating process&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a>. Recent works have exploited this phenomenon by comparing\n","the model before and after unlearning to measure the information\n","leakage. More precisely, Salem et al.&#xA0;<a href=\"#ref-salem2020updates\">(Salem et al. 2020)</a> proposed an\n","adversary attack in the image domain that could reconstruct a removed\n","sample when a classifier is unlearned on a data sample. Brockschmidt et\n","al.&#xA0;<a href=\"#ref-zanella2020analyzing\">(Zanella-B&#xE9;guelin et al. 2020)</a>\n","suggested a similar approach for the text domain. Chen et al.&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a> introduced a membership inference attack to detect whether\n","a removed sample belongs to the learning set. Compared to previous\n","works&#xA0;<a href=\"#ref-Salem0HBF019 shokri2017membership\">(Salem et al. 2019;\n","Shokri et al. 2017)</a>, their approach additionally makes use of the\n","posterior output distribution of the original model, besides that of the\n","unlearned model. Chen et al.&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al. 2021b)</a> also proposed\n","two leakage metrics, namely the degradation count and the degradation\n","rate.</p>\n","<div class=\"compactitem\">\n","<p>The <em>degradation count:</em> is defined as the ratio between the\n","number of target samples whose membership can be inferred by the\n","proposed attack with higher confidence compared to traditional attacks\n","and the total number of samples.</p>\n","<p>The <em>degradation rate:</em> is defined the average improvement\n","rate of the confidence of the proposed attack compared to traditional\n","attacks.</p>\n","</div>\n","<p><strong>Membership Inference Attacks.</strong> This kind of attack is\n","designed to detect whether a target model leaks data&#xA0;<a href=\"#ref-shokri2017membership thudi2022bounding chen2021machine\">(Shokri\n","et al. 2017; Thudi, Shumailov, et al. 2022; M. Chen et al.\n","2021b)</a>. Specifically, an inference model is trained to recognise\n","new data samples from the training data used to optimize the target\n","model. In&#xA0;<a href=\"#ref-shokri2017membership\">(Shokri et al. 2017)</a>, a set of\n","shallow models were trained on a new set of data items different from\n","the one that the target model was trained on. The attack model was then\n","trained to predict whether a data item belonged to the training data\n","based on the predictions made by shallow models for training as well as\n","testing data. The training set for the shallow and attack models share\n","similar data distribution to the target model. Membership inference\n","attacks are helpful for detecting data leaks. Hence, they are useful for\n","verifying the effectiveness of the machine unlearning&#xA0;<a href=\"#ref-chen2021machine\">(M. Chen et al.\n","2021b)</a>.</p>\n","<p><strong>Backdoor attacks.</strong> Backdoor attacks were proposed to\n","inject backdoors to the data for deceiving a machine learning\n","model&#xA0;<a href=\"#ref-wang2019neural\">(B. Wang et al.\n","2019)</a>. The deceived model makes correct predictions with clean\n","data, but with poison data in a target class as a backdoor trigger, it\n","makes incorrect predictions. Backdoor attacks were used to verify the\n","effectiveness of machine unlearning in&#xA0;<a href=\"#ref-sommer2020towards sommer2022athena\">(Sommer et al. 2020,\n","2022)</a>. Specifically, the setting begins with training a model\n","that has a mixture of clean and poison data items across all users. Some\n","of the users want their data deleted. If the users&#x2019; data are not\n","successfully deleted, the poison samples will be predicted as the target\n","class. Otherwise, the model will not predict the poison samples as the\n","target class. However, there is no absolute guarantee that this rule is\n","always correct, although one can increase the number of poison samples\n","to make this rule less likely to fail.</p>\n","<p><strong>Slow-down attacks.</strong> Some studies focus on the\n","theoretical guarantee of indistinguishability between an unlearned and a\n","retrained models. However, the practical bounds on computation costs are\n","largely neglected in these papers&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. As a result, a new threat has been introduced to machine\n","unlearning where poisoning attacks are used to slow down the unlearning\n","process. Formally, let <span\n","class=\"math inline\"><em>h</em><sub>0</sub>&#x2004;=&#x2004;<em>A</em>(<em>D</em>)</span>\n","be an initial model trained by a learning algorithm <span\n","class=\"math inline\"><em>A</em></span> on a dataset <span\n","class=\"math inline\"><em>D</em></span>. The goal of the attacker is to\n","poison a subset <span\n","class=\"math inline\"><em>D</em><sub><em>p</em><em>o</em><em>i</em><em>s</em><em>o</em><em>n</em></sub>&#x2004;&#x2282;&#x2004;<em>D</em></span>\n","such as to maximize the computation cost of removing <span\n","class=\"math inline\"><em>D</em><sub><em>p</em><em>o</em><em>i</em><em>s</em><em>o</em><em>n</em></sub></span>\n","from <span class=\"math inline\"><em>h&#x302;</em></span> using an unlearning\n","algorithm <span class=\"math inline\"><em>U</em></span>. Marchant et al.\n","&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant,\n","Rubinstein, and Alfeld 2022)</a> defined and estimated an efficient\n","computation cost for certifying removal methods. However, generalizing\n","this computation cost for different unlearning methods is still an open\n","research direction.</p>\n","<p><strong>Interclass Confusion Test.</strong> The idea of this test is\n","to investigate whether information from the data to be forgotten can\n","still be inferred from an unlearned model&#xA0;<a href=\"#ref-goel2022evaluating\">(Goel, Prabhu, and Kumaraguru\n","2022)</a>. Different from traditional approximate unlearning\n","definitions that focus on the indistinguishability between unlearned and\n","retrained models in the parameter space, this test focuses on the output\n","space. More precisely, the test involves randomly selecting a set of\n","samples <span class=\"math inline\"><em>S</em>&#x2004;&#x2282;&#x2004;<em>D</em></span> from\n","two chosen classes in the training data <span\n","class=\"math inline\"><em>D</em></span> and then randomly swapping the\n","label assignment between the samples of different classes to result in a\n","confused set <span class=\"math inline\"><em>S</em>&#x2032;</span>. Together\n","<span class=\"math inline\"><em>S</em>&#x2032;</span> and <span\n","class=\"math inline\"><em>D</em>&#x2005;\\&#x2005;<em>S</em></span> form a new training\n","dataset <span class=\"math inline\"><em>D</em>&#x2032;</span>, resulting in a new\n","trained model. <span class=\"math inline\"><em>S</em>&#x2032;</span> is\n","considered to be the forgotten data. From this, Goet et al.&#xA0;<a href=\"#ref-goel2022evaluating\">(Goel, Prabhu, and\n","Kumaraguru 2022)</a> computes a forgetting score from a confusion\n","matrix generated by the unlearned model. A lower forgetting score means\n","a better unlearned model.</p>\n","<p><strong>Federated verification.</strong> Unlearning verification in\n","federated learning is uniquely challenging. First, the participation of\n","one or a few clients in the federation may subtly change the global\n","model&#x2019;s performance, making verification in the output space\n","challenging. Second, verification using adversarial attacks is not\n","applicable in the federated setting because it might introduce new\n","security threats to the infrastructure&#xA0;<a href=\"#ref-gao2022verifi\">(X. Gao et al. 2022)</a>. As a result, Gao\n","et al.&#xA0;<a href=\"#ref-gao2022verifi\">(X. Gao et al.\n","2022)</a> proposes a verification mechanism that uses a few\n","communication rounds for clients to verify their data in the global\n","model. This approach is compatible with federated settings because the\n","model is trained in the same way where the clients communicate with the\n","server over several rounds.</p>\n","<p><strong>Cryptographic proofs.</strong> Since most of existing\n","verification frameworks do not provide any theoretical guarantee,\n","Eisenhofer et al.&#xA0;<a href=\"#ref-eisenhofer2022verifiable\">(Eisenhofer et al. 2022)</a>\n","proposed a cryptography-informed protocol to compute two proofs,\n","i.e.&#xA0;proof of update (the model was trained on a particular dataset\n","<span class=\"math inline\"><em>D</em></span>) and proof of unlearning\n","(the forget item <span class=\"math inline\"><em>d</em></span> is not a\n","member of <span class=\"math inline\"><em>D</em></span>). The core idea of\n","the proof of update is using SNARK&#xA0;<a href=\"#ref-bitansky2012extractable\">(Bitansky et al. 2012)</a> data\n","structure to commit a hash whenever the model is updated (learned or\n","unlearned) while ensuring that: (i) the model was obtained from the\n","remaining data, (ii) the remaining data does not contain any forget\n","items, (iii) the previous forget set is a subset of the current forget\n","set, and (iv) the forget items are never re-added into the training\n","data. The core idea of the proof of unlearning is using the Merkle tree\n","to maintain the order of data items in the training data so that an\n","unlearned item cannot be added to the training data again. While the\n","approach is demonstrated on SISA (efficient retraining)&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>, it is applicable for any unlearning method.</p>\n","<h1 id=\"sec:problem\">Unlearning Definition</h1>\n","<h2 id=\"sec:formulation\">Problem Formulation</h2>\n","<p>While the application of machine unlearning can originate from\n","security, usability, fidelity, and privacy reasons, it is often\n","formulated as a privacy preserving problem where users can ask for the\n","removal of their data from computer systems and machine learning\n","models&#xA0;<a href=\"#ref-sekhari2021remember ginart2019making bourtoule2021machine garg2020formalizing\">(Sekhari\n","et al. 2021; Ginart et al. 2019; Bourtoule et al. 2021; Garg,\n","Goldwasser, and Vasudevan 2020)</a>. The forgetting request can be\n","motivated by security and usability reasons as well. For example, the\n","models can be attacked by adversarial data and produce wrong outputs.\n","Once these types of attacks are detected, the corresponding adversarial\n","data has to be removed as well without harming the model&#x2019;s predictive\n","performance.</p>\n","<p>When fulfilling a removal request, the computer system needs to\n","remove all user&#x2019;s data and &#x2018;forget&#x2019; any influence on the models that\n","were trained on those data. As removing data from a database is\n","considered trivial, the literature mostly concerns how to unlearn data\n","from a model&#xA0;<a href=\"#ref-GuoGHM20 izzo2021approximate neel2021descent ullah2021machine\">(C.\n","Guo et al. 2020; Izzo et al. 2021; Neel, Roth, and Sharifi-Malvajerdi\n","2021; Ullah et al. 2021)</a>.</p>\n","<p>To properly formulate an unlearning problem, we need to introduce a\n","few concepts. First, let us denote <span class=\"math inline\">&#x1D4B5;</span> as\n","an example space, i.e., a space of data items or examples (called\n","samples). Then, the set of all possible training datasets is denoted as\n","<span class=\"math inline\">&#x1D4B5;<sup>*</sup></span>. One can argue that <span\n","class=\"math inline\">&#x1D4B5;<sup>*</sup>&#x2004;=&#x2004;2<sup>&#x1D4B5;</sup></span> but that is not\n","important, as a particular training dataset <span\n","class=\"math inline\"><em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup></span> is often\n","given as input. Given <span class=\"math inline\"><em>D</em></span>, we\n","want to get a machine learning model from a hypothesis space <span\n","class=\"math inline\">&#x210B;</span>. In general, the hypothesis space <span\n","class=\"math inline\">&#x210B;</span> covers the parameters and the meta-data of\n","the models. Sometimes, it is modeled as <span\n","class=\"math inline\">&#x1D4B2;&#x2005;&#xD7;&#x2005;<em>&#x398;</em></span>, where <span\n","class=\"math inline\">&#x1D4B2;</span> is the parameter space and <span\n","class=\"math inline\"><em>&#x398;</em></span> is the metadata/state space. The\n","process of training a model on <span\n","class=\"math inline\"><em>D</em></span> in the given computer system is\n","enabled by a learning algorithm, denoted by a function <span\n","class=\"math inline\"><em>A</em>&#x2004;:&#x2004;&#x1D4B5;<sup>*</sup>&#x2004;&#x2192;&#x2004;&#x210B;</span>, with the\n","trained model denoted as <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span>.</p>\n","<p>To support forgetting requests, the computer system needs to have an\n","unlearning mechanism, denoted by a function <span\n","class=\"math inline\"><em>U</em></span>, that takes as input a training\n","dataset <span\n","class=\"math inline\"><em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup></span>, a forget\n","set <span\n","class=\"math inline\"><em>D</em><sub><em>f</em></sub>&#x2004;&#x2282;&#x2004;<em>D</em></span>\n","(data to forget) and a model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span>. It returns a\n","sanitized (or unlearned) model <span\n","class=\"math inline\"><em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))&#x2004;&#x2208;&#x2004;&#x210B;</span>.\n","The unlearned model is expected to be the same or similar to a retrained\n","model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>\\<em>D</em><sub><em>f</em></sub>)</span>\n","(i.e., a model as if it had been trained on the remaining data). Note\n","that <span class=\"math inline\"><em>A</em></span> and <span\n","class=\"math inline\"><em>U</em></span> are assumed to be randomized\n","algorithms, i.e., the output is non-deterministic and can be modelled as\n","a conditional probability distribution over the hypothesis space given\n","the input data&#xA0;<a href=\"#ref-marchant2022hard\">(Marchant, Rubinstein, and Alfeld\n","2022)</a>. This assumption is reasonable as many learning algorithms\n","are inherently stochastic (e.g., SGD) and some floating-point operations\n","involve randomness in computer implementations&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al. 2021)</a>.\n","Another note is that we do not define the function <span\n","class=\"math inline\"><em>U</em></span> precisely before-hand as its\n","definition varies with different settings.</p>\n","<p><a href=\"#tab:symbols\" data-reference-type=\"autoref\"\n","data-reference=\"tab:symbols\">[tab:symbols]</a> summarizes important\n","notations.</p>\n","<div class=\"adjustbox\">\n","<p>max width=0.45</p>\n","<div id=\"tab:symbols\">\n","<table>\n","<caption>Important notations</caption>\n","<thead>\n","<tr class=\"header\">\n","<th style=\"text-align: center;\"><strong>Symbols</strong></th>\n","<th style=\"text-align: left;\"><strong>Definition</strong></th>\n","</tr>\n","</thead>\n","<tbody>\n","<tr class=\"odd\">\n","<td style=\"text-align: center;\"><span class=\"math inline\">&#x1D4B5;</span></td>\n","<td style=\"text-align: left;\">example space</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>D</em></span></td>\n","<td style=\"text-align: left;\">the training dataset</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>D</em><sub><em>f</em></sub></span></td>\n","<td style=\"text-align: left;\">forgetting set (the data to be\n","forgotten)</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>D</em><sub><em>r</em></sub>&#x2004;=&#x2004;<em>D</em>&#x2005;\\&#x2005;<em>D</em><sub><em>f</em></sub></span></td>\n","<td style=\"text-align: left;\">retained set (the remaining data)</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>A</em>(.)</span></td>\n","<td style=\"text-align: left;\">a learning algorithm</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>U</em>(.)</span></td>\n","<td style=\"text-align: left;\">an unlearning algorithm</td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: center;\"><span class=\"math inline\">&#x210B;</span></td>\n","<td style=\"text-align: left;\">hypothesis space of models</td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>w</em>&#x2004;=&#x2004;<em>A</em>(<em>D</em>)</span></td>\n","<td style=\"text-align: left;\">Parameters of the model trained on <span\n","class=\"math inline\"><em>D</em></span> by <span\n","class=\"math inline\"><em>A</em></span></td>\n","</tr>\n","<tr class=\"odd\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>w</em><sub><em>r</em></sub>&#x2004;=&#x2004;<em>A</em>(<em>D</em><sub><em>r</em></sub>)</span></td>\n","<td style=\"text-align: left;\">Parameters of the model trained on <span\n","class=\"math inline\"><em>D</em><sub><em>r</em></sub></span> by <span\n","class=\"math inline\"><em>A</em></span></td>\n","</tr>\n","<tr class=\"even\">\n","<td style=\"text-align: center;\"><span\n","class=\"math inline\"><em>w</em><sub><em>u</em></sub>&#x2004;=&#x2004;<em>U</em>(.)</span></td>\n","<td style=\"text-align: left;\">Parameters of the model unlearned by <span\n","class=\"math inline\"><em>U</em>(.)</span></td>\n","</tr>\n","</tbody>\n","</table>\n","</div>\n","</div>\n","<h2 id=\"sec:exact_unlearning\">Exact Unlearning (Perfect Unlearning)</h2>\n","<p>The core problem of machine unlearning involves the comparison\n","between two distributions of machine learning models&#xA0;<a href=\"#ref-thudi2022unrolling bourtoule2021machine brophy2021machine\">(Thudi,\n","Deza, et al. 2022; Bourtoule et al. 2021; Brophy and Lowd 2021)</a>.\n","Let <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>A</em>(<em>D</em>))</span>\n","define the distribution of all models trained on a dataset <span\n","class=\"math inline\"><em>D</em></span> by a learning algorithm <span\n","class=\"math inline\"><em>A</em>(.)</span>. Let <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>)))</span>\n","be the distribution of unlearned models. The reason why the output of\n","<span class=\"math inline\"><em>U</em>(.)</span> is modelled as a\n","distribution rather than a single point is that learning algorithms\n","<span class=\"math inline\"><em>A</em>(.)</span> and unlearning algorithms\n","<span class=\"math inline\"><em>U</em>(.)</span> are randomized as\n","mentioned above.</p>\n","<div id=\"def:exact_special\" class=\"dfn\">\n","<p><strong>Definition 1</strong> (Exact unlearning - special case).\n","<em>Given a learning algorithm <span\n","class=\"math inline\"><em>A</em>(.)</span>, a dataset <span\n","class=\"math inline\"><em>D</em></span>, and a forget set <span\n","class=\"math inline\"><em>D</em><sub><em>f</em></sub>&#x2004;&#x2286;&#x2004;<em>D</em></span>,\n","we say the process <span class=\"math inline\"><em>U</em>(.)</span> is an\n","exact unlearning process iff: <span\n","class=\"math display\"><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>D</em><sub><em>f</em></sub>))&#x2004;=&#x2004;<em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>)))</span></em></p>\n","</div>\n","<p>Two key aspects can be drawn from this definition. First, the\n","definition does not require that the model <span\n","class=\"math inline\"><em>A</em>(<em>D</em>)</span> be retrained from\n","scratch on <span\n","class=\"math inline\"><em>D</em>&#x2005;\\&#x2005;<em>D</em><sub><em>f</em></sub></span>.\n","Rather, it requires some evidence that it is likely to be a model that\n","is trained from scratch on <span\n","class=\"math inline\"><em>D</em>&#x2005;\\&#x2005;<em>D</em><sub><em>f</em></sub></span>.\n","Second, two models trained with the same dataset should belong to the\n","same distribution. However, defining this distribution is tricky. So to\n","avoid the unlearning algorithm being specific to a particular training\n","dataset, we have a more general definition&#xA0;<a href=\"#ref-ginart2019making brophy2021machine\">(Ginart et al. 2019;\n","Brophy and Lowd 2021)</a>:</p>\n","<div id=\"def:exact_general\" class=\"dfn\">\n","<p><strong>Definition 2</strong> (Exact unlearning - general case).\n","<em>Given a learning algorithm <span\n","class=\"math inline\"><em>A</em>(.)</span>, we say the process <span\n","class=\"math inline\"><em>U</em>(.)</span> is an exact unlearning process\n","iff <span\n","class=\"math inline\">&#x2200;&#x1D4AF;&#x2004;&#x2286;&#x2004;&#x210B;,&#x2006;<em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup>,&#x2006;<em>D</em><sub><em>f</em></sub>&#x2004;&#x2282;&#x2004;<em>D</em></span>:\n","<span\n","class=\"math display\"><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>D</em><sub><em>f</em></sub>)&#x2208;&#x1D4AF;)&#x2004;=&#x2004;<em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)</span></em></p>\n","</div>\n","<p>This definition allows us to define a metric space the models belong\n","to (and consequently for the distributions). A model can be viewed\n","either as just a mapping of inputs to outputs in which case <span\n","class=\"math inline\"><em>P</em><em>r</em>(.)</span> are distributions\n","over a function space (i.e., continuous function with the supremum\n","metric), or as the specific parameters <span\n","class=\"math inline\"><strong>&#x3B8;</strong></span> for a model architecture,\n","in which case <span class=\"math inline\"><em>P</em><em>r</em>(.)</span>\n","are distributions over the weight space (e.g., some finite dimensional\n","real vector space with the Euclidean norm). This ambiguity leads to two\n","notions of exact unlearning:</p>\n","<ul>\n","<li><p><em>Distribution of weights</em>: <a href=\"#eq:exact_general\"\n","data-reference-type=\"autoref\"\n","data-reference=\"eq:exact_general\">[eq:exact_general]</a> implies the\n","zero difference in the distribution of weights, i.e., <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>w</em><sub><em>r</em></sub>)&#x2004;=&#x2004;<em>P</em><em>r</em>(<em>w</em><sub><em>u</em></sub>)</span>,\n","where the parameters of models <span\n","class=\"math inline\"><em>w</em><sub><em>r</em></sub></span> learned by\n","<span\n","class=\"math inline\"><em>A</em>(<em>D</em><sub><em>r</em></sub>)</span>\n","and <span class=\"math inline\"><em>w</em><sub><em>u</em></sub></span> are\n","the parameters of the models given by <span\n","class=\"math inline\"><em>U</em>(.)</span>.</p></li>\n","<li><p><em>Distribution of outputs:</em> <a href=\"#eq:exact_general\"\n","data-reference-type=\"autoref\"\n","data-reference=\"eq:exact_general\">[eq:exact_general]</a> implies zero\n","difference in the distribution of outputs, i.e., <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>M</em>(<em>X</em>;<em>w</em><sub><em>r</em></sub>))</span>\n","= <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>M</em>(<em>X</em>;<em>w</em><sub><em>u</em></sub>))</span>,\n","<span class=\"math inline\">&#x2200;<em>X</em>&#x2004;&#x2286;&#x2004;&#x1D4B5;</span>, where <span\n","class=\"math inline\"><em>M</em>(.)</span> is the parameterized mapping\n","function from the input space <span class=\"math inline\">&#x1D4B5;</span> to the\n","output space (i.e., the machine learning model). This definition is\n","sometimes referred to as <em>weak unlearning</em>&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a>.</p></li>\n","</ul>\n","<p>If the unlearning mechanism <span\n","class=\"math inline\"><em>U</em>(.)</span> is implemented as retraining\n","itself, equality is absolutely guaranteed. For this reason, retraining\n","is sometimes considered to be the only exact unlearning method. However,\n","retraining inherently involves high computation costs, especially for\n","large models&#xA0;<a href=\"#ref-thudi2022unrolling\">(Thudi, Deza, et al. 2022)</a>.\n","Another disadvantage of retraining is that it cannot deal with batch\n","settings, where multiple removal requests happen simultaneously or are\n","grouped in a batch.</p>\n","<p>There are many different metrics for comparing numerical\n","distributions over the output space and the weight space. However, doing\n","so is expensive (e.g., generating a sample in these distributions\n","involves training the whole model). To mitigate this issue, some\n","approaches design an alternative metric on a point basis to compute the\n","distance between two models, either in the output space or in the weight\n","space&#xA0;<a href=\"#ref-shokri2017membership\">(Shokri\n","et al. 2017)</a>.</p>\n","<h2 id=\"approximate-unlearning-boundedcertified-unlearning\">Approximate\n","Unlearning (Bounded/Certified Unlearning)</h2>\n","<p>Approximate unlearning approaches attempt to address these\n","cost-related constraints. In lieu of retraining, these strategies:\n","perform computationally less costly actions on the final weights&#xA0;<a href=\"#ref-GuoGHM20 graves2021amnesiac sekhari2021remember\">(C. Guo et\n","al. 2020; Graves, Nagisetty, and Ganesh 2021; Sekhari et al.\n","2021)</a>; modify the architecture&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a>; or filter the outputs&#xA0;<a href=\"#ref-baumhauer2020machine\">(Baumhauer, Sch&#xF6;ttle, and Zeppelzauer\n","2020)</a>. Approximate unlearning relaxes <a\n","href=\"#def:exact_general\" data-reference-type=\"autoref\"\n","data-reference=\"def:exact_general\">[def:exact_general]</a> as\n","follows&#xA0;<a href=\"#ref-GuoGHM20\">(C. Guo et al.\n","2020)</a>.</p>\n","<div class=\"definition\">\n","<p>Given <span class=\"math inline\"><em>&#x3F5;</em>&#x2004;&gt;&#x2004;0</span>, an\n","unlearning mechanism <span class=\"math inline\"><em>U</em></span>\n","performs <span class=\"math inline\"><em>&#x3F5;</em></span>-certified removal\n","for a learning algorithm <span class=\"math inline\"><em>A</em></span> if\n","<span\n","class=\"math inline\">&#x2200;&#x1D4AF;&#x2004;&#x2286;&#x2004;&#x210B;,&#x2006;<em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup>,&#x2006;<em>z</em>&#x2004;&#x2208;&#x2004;<em>D</em></span>:\n","<span class=\"math display\">$$\\label{eq:approximate_epsilon}\n","e^{-\\epsilon}\\leq \\frac{Pr( U(D, z, A(D)) \\in \\mathcal{T})}{Pr(A(D\n","\\setminus z) \\in \\mathcal{T})} \\leq e^{\\epsilon}$$</span> where <span\n","class=\"math inline\"><em>z</em></span> is the removed sample.</p>\n","</div>\n","<p>It is noteworthy that <a href=\"#eq:approximate_epsilon\"\n","data-reference-type=\"autoref\"\n","data-reference=\"eq:approximate_epsilon\">[eq:approximate_epsilon]</a>\n","defines the bounds on a single sample <span\n","class=\"math inline\"><em>z</em></span> only. It is still an open question\n","as to whether constant bounds can be provided for bigger subsets of\n","<span class=\"math inline\"><em>D</em></span>. Moreover, the reason why we\n","have the <span\n","class=\"math inline\">[<em>e</em><sup>&#x2212;<em>&#x3F5;</em></sup>,<em>e</em><sup><em>&#x3F5;</em></sup>]</span>\n","bounds is that the probability distributions are often modeled by log\n","functions, in which <a href=\"#eq:approximate_epsilon\"\n","data-reference-type=\"autoref\"\n","data-reference=\"eq:approximate_epsilon\">[eq:approximate_epsilon]</a> is\n","equivalent to: <span\n","class=\"math display\">&#x2005;&#x2212;&#x2005;<em>&#x3F5;</em>&#x2004;&#x2264;&#x2004;log&#x2006;[<em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>z</em>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)&#x2212;<em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>z</em>)&#x2208;&#x1D4AF;)]&#x2004;&#x2264;&#x2004;<em>&#x3F5;</em></span>\n","or: <span\n","class=\"math display\">log&#x2006;||<em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>z</em>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)&#x2005;&#x2212;&#x2005;<em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>z</em>)&#x2208;&#x1D4AF;)||&#x2004;&#x2264;&#x2004;<em>&#x3F5;</em></span>\n","where <span class=\"math inline\">||.||</span> is an absolute distance\n","metric on the weight space or the output space. A relaxed version of\n","<span class=\"math inline\"><em>&#x3F5;</em></span>-approximate unlearning is\n","also defined in&#xA0;<a href=\"#ref-neel2021descent\">(Neel, Roth, and Sharifi-Malvajerdi\n","2021)</a>:</p>\n","<div class=\"dfn\">\n","<p><strong>Definition 3</strong> ((<span\n","class=\"math inline\"><em>&#x3F5;</em></span>,<span\n","class=\"math inline\"><em>&#x3B4;</em></span>)-Approximate Unlearning).\n","<em>Given <span\n","class=\"math inline\"><em>&#x3F5;</em>,&#x2006;<em>&#x3B4;</em>&#x2004;&gt;&#x2004;0</span>, an unlearning\n","mechanism <span class=\"math inline\"><em>U</em></span> performs <span\n","class=\"math inline\"><em>&#x3F5;</em></span>-certified removal for a learning\n","algorithm <span class=\"math inline\"><em>A</em></span> if <span\n","class=\"math inline\">&#x2200;&#x1D4AF;&#x2004;&#x2286;&#x2004;&#x210B;,&#x2006;<em>D</em>&#x2004;&#x2208;&#x2004;<em>Z</em><sup>*</sup>,&#x2006;<em>z</em>&#x2004;&#x2208;&#x2004;<em>D</em></span>:\n","<span\n","class=\"math display\"><em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>z</em>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)&#x2004;&#x2264;&#x2004;<em>e</em><sup><em>&#x3F5;</em></sup><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>z</em>)&#x2208;&#x1D4AF;)&#x2005;+&#x2005;<em>&#x3B4;</em></span>\n","and <span\n","class=\"math display\"><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>z</em>)&#x2208;&#x1D4AF;)&#x2004;&#x2264;&#x2004;<em>e</em><sup><em>&#x3F5;</em></sup><em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>z</em>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)&#x2005;+&#x2005;<em>&#x3B4;</em></span></em></p>\n","</div>\n","<p>In other words, <span class=\"math inline\"><em>&#x3B4;</em></span> upper\n","bounds the probability for the max-divergence bound in <a\n","href=\"#eq:approximate_epsilon\" data-reference-type=\"autoref\"\n","data-reference=\"eq:approximate_epsilon\">[eq:approximate_epsilon]</a> to\n","fail.</p>\n","<p><strong>Relationship to differential privacy.</strong> Differential\n","privacy states that: <span class=\"math display\">$$\\forall \\mathcal{T}\n","\\subseteq \\mathcal{H}, D, D': e^{-\\epsilon} \\leq \\frac{Pr(A(D) \\in\n","\\mathcal{T})}{Pr(A(D \\setminus z) \\in \\mathcal{T})} \\leq\n","e^\\epsilon$$</span> where <span class=\"math inline\"><em>z</em></span> is\n","the removed sample. Differential privacy implies approximate unlearning:\n","deleting the training data is not a concern if algorithm <span\n","class=\"math inline\"><em>A</em></span> never memorises it in the first\n","place&#xA0;<a href=\"#ref-GuoGHM20\">(C. Guo et al.\n","2020)</a>. However, this is exactly the contradiction between\n","differential privacy and machine unlearning. If <span\n","class=\"math inline\"><em>A</em></span> is differentially private for any\n","data, then it does not learn anything from the data itself&#xA0;<a href=\"#ref-bourtoule2021machine\">(Bourtoule et al.\n","2021)</a>. In other words, differential privacy is a very strong\n","condition, and most differentially private models suffer a significant\n","loss in accuracy even for large <span\n","class=\"math inline\"><em>&#x3F5;</em></span>&#xA0;<a href=\"#ref-chaudhuri2011differentially abadi2016deep\">(Chaudhuri,\n","Monteleoni, and Sarwate 2011; Abadi et al. 2016)</a>.</p>\n","<h2 id=\"indistinguishability-metrics\">Indistinguishability Metrics</h2>\n","<p>To compare the two models in <a href=\"#def:exact_general\"\n","data-reference-type=\"autoref\"\n","data-reference=\"def:exact_general\">[def:exact_general]</a>, we need to\n","define a distance metric <span class=\"math inline\"><em>d</em>(.)</span>\n","between <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>A</em>(<em>D</em>\\<em>D</em><sub><em>f</em></sub>)&#x2208;&#x1D4AF;)</span>\n","and <span\n","class=\"math inline\"><em>P</em><em>r</em>(<em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))&#x2208;&#x1D4AF;)</span>\n","(<span class=\"math inline\">&#x2200;&#x1D4AF;&#x2004;&#x2286;&#x2004;&#x210B;</span>) in either the weight\n","(parameter) space or the output space. To this end, several distance\n","metrics have been studied:</p>\n","<p><strong><span\n","class=\"math inline\">&#x2113;<sub>2</sub></span>-distance.</strong> Wu et\n","al.&#xA0;<a href=\"#ref-wu2020deltagrad\">(Y. Wu et al.\n","2020)</a> proposed using a Euclidean norm to compare the weights of\n","<span\n","class=\"math inline\"><em>A</em>(<em>D</em><sub><em>r</em></sub>)</span>\n","and the weights of <span\n","class=\"math inline\"><em>U</em>(<em>D</em>,<em>D</em><sub><em>f</em></sub>,<em>A</em>(<em>D</em>))</span>.\n","This is also termed as <em>verification error</em>&#xA0;<a href=\"#ref-wu2020deltagrad\">(Y. Wu et al.\n","2020)</a>. Despite being simple, this metric has several limitations:\n","(1) It is costly to compute this verification error as we need to also\n","calculate <span\n","class=\"math inline\"><em>A</em>(<em>D</em><sub><em>r</em></sub>)</span>\n","(through naive retraining). If the computational cost is cheap, machine\n","unlearning is not necessary in the first place. (2) It is possible for\n","two models having same training set and initialisation to have different\n","weights&#xA0;<a href=\"#ref-jia2021proof\">(Jia et al.\n","2021)</a> due to training stochasticity and uncertainties in\n","floating-point operations. Therefore, it is quite tricky to define a\n","threshold for this error.</p>\n","<p><strong>KL Divergence.</strong> The Kullback-Leiber (KL) divergence\n","or the Jensen-Shannon divergence is a popular distance metric between\n","two distributions. Golatkar et al.&#xA0;<a href=\"#ref-golatkar2020eternal\">(Golatkar, Achille, and Soatto\n","2020a)</a> considered this divergence to measure the distance between\n","two models in the parameter space. Although it might not require\n","computing <span\n","class=\"math inline\"><em>A</em>(<em>D</em><sub><em>r</em></sub>)</span>,\n","it is necessary to have final distributions of models train on <span\n","class=\"math inline\"><em>D</em><sub><em>r</em></sub></span> for computing\n","the divergence. Distribution modeling is non-trivial and might involve\n","sampling of many models trained on <span\n","class=\"math inline\"><em>D</em><sub><em>r</em></sub></span> as well.</p>\n","<p><strong>Weight Leakage.</strong> Some studies measure privacy leaks\n","of the removed sample <span class=\"math inline\"><em>z</em></span> from\n","the parameters of the unlearned model&#xA0;<a href=\"#ref-dwork2014algorithmic sekhari2021remember GuoGHM20\">(Dwork,\n","Roth, et al. 2014; Sekhari et al. 2021; C. Guo et al. 2020)</a>.\n","These works assume that a model&#x2019;s weight distribution does not leak\n","information about <span class=\"math inline\"><em>z</em></span> if it was\n","not trained on <span class=\"math inline\"><em>z</em></span>. However,\n","measuring this privacy leakage is non-trivial and only well-defined for\n","special classes of models. Guo et al.&#xA0;<a href=\"#ref-GuoGHM20\">(C. Guo et al. 2020)</a> proposed such a metric\n","for linear classifiers via gradients. More precisely, a model <span\n","class=\"math inline\"><em>w</em><sup>*</sup></span> is trained on <span\n","class=\"math inline\"><em>D</em></span> if the gradient <span\n","class=\"math inline\">&#x2207;<em>L</em>(<em>D</em>;<em>w</em><sup>*</sup>)&#x2004;=&#x2004;0</span>,\n","where <span class=\"math inline\"><em>L</em>(.)</span> is an empirical\n","risk (loss) of a linear model. This is because <span\n","class=\"math inline\">arg&#x2006;min<sub><em>w</em></sub><em>L</em>(<em>D</em>;<em>w</em>)</span>\n","is uniquely determined for such <span\n","class=\"math inline\"><em>L</em>(.)</span>. As a result, if the gradient\n","<span\n","class=\"math inline\">&#x2207;<em>L</em>(<em>D</em>\\<em>z</em>;<em>w</em><sub><em>u</em></sub>)</span>,\n","where <span class=\"math inline\"><em>w</em><sub><em>u</em></sub></span>\n","is the parameters of the model returned by <span\n","class=\"math inline\"><em>U</em></span>, is non-zero then the model either\n","does not finish training or is not trained on <span\n","class=\"math inline\"><em>D</em>&#x2005;\\&#x2005;<em>z</em></span>. The former case can\n","be safely ignored as we are not interested in non-converged models.\n","However, the latter case indirectly implies that the model was trained\n","on a dataset that included <span class=\"math inline\"><em>z</em></span>,\n","and thus it reveals some information about <span\n","class=\"math inline\"><em>z</em></span>. As a result, the gradient <span\n","class=\"math inline\">&#x2207;<em>L</em>(<em>D</em>\\<em>z</em>;<em>w</em><sub><em>u</em></sub>)</span>\n","becomes an objective function for minimizing (or providing a bound) in\n","those works&#xA0;<a href=\"#ref-sekhari2021remember GuoGHM20\">(Sekhari et al. 2021; C. Guo\n","et al. 2020)</a>. Although this metric does provide an efficient way\n","to verify the unlearning result, the above assumptions are not always\n","correct from a numerical perspective, especially for non-linear\n","models&#xA0;<a href=\"#ref-gupta2021adaptive\">(Gupta et\n","al. 2021)</a>. Using these metrics also requires access to the\n","remaining data as well as the loss function, which are not always\n","available.</p>"]},{"cell_type":"markdown","id":"e639f259","metadata":{"papermill":{"duration":0.004275,"end_time":"2023-07-05T23:39:33.392757","exception":false,"start_time":"2023-07-05T23:39:33.388482","status":"completed"},"tags":[]},"source":["### Model-Agnostic Approaches\n","[![Model-Agnostic](https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/figs/model-agnostic.png)](https://arxiv.org/abs/2209.02299)\n","Model-agnostic machine unlearning methodologies include unlearning processes or frameworks that are applicable for different models. In some cases, they provide theoretical guarantees for only a class of models (e.g. linear models). But we still consider them model-agnostic as their core ideas are applicable to complex models (e.g. deep neural networks) with practical results.\n","\n","| **Paper Title** | **Year** | **Author** | **Venue** | **Model** | **Code** |\n","| --------------- | :----: | ---- | :----: | :----: | :----: |\n","| [Towards Adversarial Evaluations for Inexact Machine Unlearning](https://arxiv.org/abs/2201.06640) | 2023 | Goel et al. | _arXiv_ | EU-k, CF-k | [[Code]](https://github.com/shash42/Evaluating-Inexact-Unlearning) |\n","| [On the Trade-Off between Actionable Explanations and the Right to be Forgotten](https://openreview.net/pdf?id=HWt4BBZjVW) | 2023 | Pawelczyk et al. | _arXiv_ | - | - |  |\n","| [Towards Unbounded Machine Unlearning](https://arxiv.org/pdf/2302.09880) | 2023 | Kurmanji et al. | _arXiv_ | SCRUB | [[Code]](https://github.com/Meghdad92/SCRUB) | approximate unlearning |\n","| [Netflix and Forget: Efficient and Exact Machine Unlearning from Bi-linear Recommendations](https://arxiv.org/abs/2302.06676) | 2023 | Xu et al. | _arXiv_ | Unlearn-ALS | - | Exact Unlearning |\n","| [To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods](https://arxiv.org/abs/2302.03350) | 2023 | Zhang et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/machine-unlearning) | |\n","| [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization](https://arxiv.org/abs/2211.11656) | 2022 | Fraboni et al. | _arXiv_ | SIFU | - | |\n","| [Certified Data Removal in Sum-Product Networks](https://arxiv.org/abs/2210.01451) | 2022 | Becker and Liebig | _ICKG_ | UNLEARNSPN | [[Code]](https://github.com/ROYALBEFF/UnlearnSPN) | Certified Removal Mechanisms |\n","| [Learning with Recoverable Forgetting](https://arxiv.org/abs/2207.08224) | 2022 | Ye et al.  | _ECCV_ | LIRF | - |  |\n","| [Continual Learning and Private Unlearning](https://arxiv.org/abs/2203.12817) | 2022 | Liu et al. | _CoLLAs_ | CLPU | [[Code]](https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning) | |\n","| [Verifiable and Provably Secure Machine Unlearning](https://arxiv.org/abs/2210.09126) | 2022 | Eisenhofer et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/verifiable-unlearning) |  Certified Removal Mechanisms |\n","| [VeriFi: Towards Verifiable Federated Unlearning](https://arxiv.org/abs/2205.12709) | 2022 | Gao et al. | _arXiv_ | VERIFI | - | Certified Removal Mechanisms |\n","| [FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information](https://arxiv.org/abs/2210.10936) | 2022 | Cao et al. | _S&P_ | FedRecover | - | recovery method |\n","| [Fast Yet Effective Machine Unlearning](https://arxiv.org/abs/2111.08947) | 2022 | Tarun et al. | _arXiv_ | UNSIR | - |  |\n","| [Membership Inference via Backdooring](https://arxiv.org/abs/2206.04823) | 2022 | Hu et al.  | _IJCAI_ | MIB | [[Code]](https://github.com/HongshengHu/membership-inference-via-backdooring) | Membership Inferencing |\n","| [Forget Unlearning: Towards True Data-Deletion in Machine Learning](https://arxiv.org/abs/2210.08911) | 2022 | Chourasia et al. | _ICLR_ | - | - | noisy gradient descent |\n","| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | Chundawat et al. | _arXiv_ | - | - |  |\n","| [Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations](https://arxiv.org/abs/2202.13295) | 2022 | Guo et al. | _arXiv_ | attribute unlearning | - |  |\n","| [Few-Shot Unlearning](https://download.huan-zhang.com/events/srml2022/accepted/yoon22fewshot.pdf) | 2022 | Yoon et al.   | _ICLR_ | - | - |  |\n","| [Federated Unlearning: How to Efficiently Erase a Client in FL?](https://arxiv.org/abs/2207.05521) | 2022 | Halimi et al. | _UpML Workshop_ | - | - | federated learning |\n","| [Machine Unlearning Method Based On Projection Residual](https://arxiv.org/abs/2209.15276) | 2022 | Cao et al. | _DSAA_ | - | - |  Projection Residual Method |\n","| [Hard to Forget: Poisoning Attacks on Certified Machine Unlearning](https://ojs.aaai.org/index.php/AAAI/article/view/20736) | 2022 | Marchant et al. | _AAAI_ | - | [[Code]](https://github.com/ngmarchant/attack-unlearning) | Certified Removal Mechanisms |\n","| [Athena: Probabilistic Verification of Machine Unlearning](https://web.archive.org/web/20220721061150id_/https://petsymposium.org/popets/2022/popets-2022-0072.pdf) | 2022 | Sommer et al. | _PoPETs_ | ATHENA | - | |\n","| [FP2-MIA: A Membership Inference Attack Free of Posterior Probability in Machine Unlearning](https://link.springer.com/chapter/10.1007/978-3-031-20917-8_12) | 2022 | Lu et al. | _ProvSec_ | FP2-MIA | - | inference attack |\n","| [Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning](https://arxiv.org/abs/2202.03460) | 2022 | Gao et al. | _PETS_ | - | - |  |\n","| [Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization](https://openreview.net/pdf?id=ue4gP8ZKiWb) | 2022 | Zhang et al.   | _NeurIPS_ | PCMU | - | Certified Removal Mechanisms |\n","| [The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining](https://arxiv.org/abs/2203.07320) | 2022 | Liu et al. | _INFOCOM_ | - | [[Code]](https://github.com/yiliucs/federated-unlearning) |  |\n","| [Backdoor Defense with Machine Unlearning](https://arxiv.org/abs/2201.09538) | 2022 | Liu et al. | _INFOCOM_ | BAERASER | - | Backdoor defense |\n","| [Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten](https://dl.acm.org/doi/abs/10.1145/3488932.3517406) | 2022 | Nguyen et al. | _ASIA CCS_ | MCU | - | MCMC Unlearning  |\n","| [Federated Unlearning for On-Device Recommendation](https://arxiv.org/abs/2210.10958) | 2022 | Yuan et al. | _arXiv_ | - | - |  |\n","| [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher](https://arxiv.org/abs/2205.08096) | 2022 | Chundawat et al. | _arXiv_ | - | - | Knowledge Adaptation |\n","| [ Efficient Two-Stage Model Retraining for Machine Unlearning](https://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.html) | 2022 | Kim and Woo | _CVPR Workshop_ | - | - |  |\n","| [Learn to Forget: Machine Unlearning Via Neuron Masking](https://ieeexplore.ieee.org/abstract/document/9844865?casa_token=_eowH3BTt1sAAAAA:X0uCpLxOwcFRNJHoo3AtA0ay4t075_cSptgTMznsjusnvgySq-rJe8GC285YhWG4Q0fUmP9Sodw0) | 2021 | Ma et al. | _IEEE_ | Forsaken | - | Mask Gradients |\n","| [Adaptive Machine Unlearning](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html) | 2021 | Gupta et al. | _NeurIPS_ | - | [[Code]](https://github.com/ChrisWaites/adaptive-machine-unlearning) | Differential Privacy |\n","| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://proceedings.mlr.press/v132/neel21a.html) | 2021 | Neel et al. | _ALT_ | - | - | Certified Removal Mechanisms |\n","| [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279) | 2021 | Sekhari et al. | _NeurIPS_ | - | - |  |\n","| [FedEraser: Enabling Efficient Client-Level Data Removal from Federated Learning Models](https://ieeexplore.ieee.org/abstract/document/9521274) | 2021 | Liu et al. | _IWQoS_ | FedEraser | - |  |\n","| [Federated Unlearning](https://arxiv.org/abs/2012.13891) | 2021 | Liu et al. | _IWQoS_ | FedEraser | [[Code]](https://www.dropbox.com/s/1lhx962axovbbom/FedEraser-Code.zip?dl=0) |  |\n","| [Machine Unlearning via Algorithmic Stability](https://proceedings.mlr.press/v134/ullah21a.html) | 2021 | Ullah et al. | _COLT_ | TV | - | Certified Removal Mechanisms |\n","| [EMA: Auditing Data Removal from Trained Models](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_76) | 2021 | Huang et al. | _MICCAI_ | EMA | [[Code]](https://github.com/Hazelsuko07/EMA) | Certified Removal Mechanisms |\n","| [Knowledge-Adaptation Priors](https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html) | 2021 | Khan and Swaroop | _NeurIPS_ | K-prior | [[Code]](https://github.com/team-approx-bayes/kpriors) | Knowledge Adaptation |\n","| [PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models](https://dl.acm.org/doi/abs/10.1145/3318464.3380571) | 2020 | Wu et al. | _NeurIPS_ | PrIU | - | Knowledge Adaptation |\n","| [Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks](https://arxiv.org/abs/1911.04933) | 2020 | Golatkar et al. | _CVPR_ | - | - | Certified Removal Mechanisms |\n","| [Learn to Forget: User-Level Memorization Elimination in Federated Learning](https://www.researchgate.net/profile/Ximeng-Liu-5/publication/340134612_Learn_to_Forget_User-Level_Memorization_Elimination_in_Federated_Learning/links/5e849e64a6fdcca789e5f955/Learn-to-Forget-User-Level-Memorization-Elimination-in-Federated-Learning.pdf) | 2020 | Liu et al. | _arXiv_ | Forsaken | - |  |\n","| [Certified Data Removal from Machine Learning Models](https://proceedings.mlr.press/v119/guo20c.html) | 2020 | Guo et al. | _ICML_ | - | - | Certified Removal Mechanisms |\n","| [Class Clown: Data Redaction in Machine Unlearning at Enterprise Scale](https://arxiv.org/abs/2012.04699) | 2020 | Felps et al. | _arXiv_ | - | - | Decremental Learning |\n","| [A Novel Online Incremental and Decremental Learning Algorithm Based on Variable Support Vector Machine](https://link.springer.com/article/10.1007/s10586-018-1772-4) | 2019 | Chen et al. | _Cluster Computing_ | - | - | Decremental Learning  |\n","| [Making AI Forget You: Data Deletion in Machine Learning](https://papers.nips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html) | 2019 | Ginart et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","| [Lifelong Anomaly Detection Through Unlearning](https://dl.acm.org/doi/abs/10.1145/3319535.3363226) | 2019 | Du et al. | _CCS_ | - | - |  |\n","| [Learning Not to Learn: Training Deep Neural Networks With Biased Data](https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Learning_Not_to_Learn_Training_Deep_Neural_Networks_With_Biased_CVPR_2019_paper.html) | 2019 | Kim et al. | _CVPR_ | - | - |  |\n","| [Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning](https://dl.acm.org/citation.cfm?id=3196517) | 2018 | Cao et al. | _ASIACCS_ | KARMA | [[Code]](https://github.com/CausalUnlearning/KARMA) |  |\n","| [Understanding Black-box Predictions via Influence Functions](https://proceedings.mlr.press/v70/koh17a.html) | 2017 | Koh et al. | _ICML_ | - | [[Code]](https://github.com/kohpangwei/influence-release) | Certified Removal Mechanisms |\n","| [Towards Making Systems Forget with Machine Unlearning](https://ieeexplore.ieee.org/abstract/document/7163042) | 2015 | Cao and Yang | _S&P_ | - |  |\n","| [Towards Making Systems Forget with Machine Unlearning](https://dl.acm.org/doi/10.1109/SP.2015.35) | 2015 | Cao et al. | _S&P_ | - | - | Statistical Query Learning  |\n","| [Incremental and decremental training for linear classification](https://dl.acm.org/doi/10.1145/2623330.2623661) | 2014 | Tsai et al. | _KDD_ | - | [[Code]](https://www.csie.ntu.edu.tw/~cjlin/papers/ws/) | Decremental Learning  |\n","| [Multiple Incremental Decremental Learning of Support Vector Machines](https://dl.acm.org/doi/10.5555/2984093.2984196) | 2009 | Karasuyama et al. | _NIPS_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Learning for Linear Support Vector Machines](https://dl.acm.org/doi/10.5555/1776814.1776838) | 2007 | Romero et al. | _ICANN_ | - | - | Decremental Learning  |\n","| [Decremental Learning Algorithms for Nonlinear Langrangian and Least Squares Support Vector Machines](https://www.semanticscholar.org/paper/Decremental-Learning-Algorithms-for-Nonlinear-and-Duan-Li/312c677f0882d0dfd60bfd77346588f52aefd10f) | 2007 | Duan et al. | _OSB_ | - | - | Decremental Learning  |\n","| [Multicategory Incremental Proximal Support Vector Classifiers](https://link.springer.com/chapter/10.1007/978-3-540-45224-9_54) | 2003 | Tveit et al. | _KES_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Proximal Support Vector Classification using Decay Coefficients](https://link.springer.com/chapter/10.1007/978-3-540-45228-7_42) | 2003 | Tveit et al. | _DaWak_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Support Vector Machine Learning](https://dl.acm.org/doi/10.5555/3008751.3008808) | 2000 | Cauwenberg et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","----------"]},{"cell_type":"markdown","id":"659cd58c","metadata":{"papermill":{"duration":0.003182,"end_time":"2023-07-05T23:39:33.400448","exception":false,"start_time":"2023-07-05T23:39:33.397266","status":"completed"},"tags":[]},"source":["### Model-Intrinsic Approaches\n","[![Model-Intrinsic](https://raw.githubusercontent.com/tamlhp/awesome-machine-unlearning/main/figs/model-intrinsic.png)](https://arxiv.org/abs/2209.02299)\n","\n","Model-agnostic machine unlearning methodologies include unlearning processes or frameworks that are applicable for different models. In some cases, they provide theoretical guarantees for only a class of models (e.g. linear models). But we still consider them model-agnostic as their core ideas are applicable to complex models (e.g. deep neural networks) with practical results.\n","\n","| **Paper Title** | **Year** | **Author** | **Venue** | **Model** | **Code** |\n","| --------------- | :----: | ---- | :----: | :----: | :----: |\n","| [Towards Adversarial Evaluations for Inexact Machine Unlearning](https://arxiv.org/abs/2201.06640) | 2023 | Goel et al. | _arXiv_ | EU-k, CF-k | [[Code]](https://github.com/shash42/Evaluating-Inexact-Unlearning) |\n","| [On the Trade-Off between Actionable Explanations and the Right to be Forgotten](https://openreview.net/pdf?id=HWt4BBZjVW) | 2023 | Pawelczyk et al. | _arXiv_ | - | - |  |\n","| [Towards Unbounded Machine Unlearning](https://arxiv.org/pdf/2302.09880) | 2023 | Kurmanji et al. | _arXiv_ | SCRUB | [[Code]](https://github.com/Meghdad92/SCRUB) | approximate unlearning |\n","| [Netflix and Forget: Efficient and Exact Machine Unlearning from Bi-linear Recommendations](https://arxiv.org/abs/2302.06676) | 2023 | Xu et al. | _arXiv_ | Unlearn-ALS | - | Exact Unlearning |\n","| [To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods](https://arxiv.org/abs/2302.03350) | 2023 | Zhang et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/machine-unlearning) | |\n","| [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization](https://arxiv.org/abs/2211.11656) | 2022 | Fraboni et al. | _arXiv_ | SIFU | - | |\n","| [Certified Data Removal in Sum-Product Networks](https://arxiv.org/abs/2210.01451) | 2022 | Becker and Liebig | _ICKG_ | UNLEARNSPN | [[Code]](https://github.com/ROYALBEFF/UnlearnSPN) | Certified Removal Mechanisms |\n","| [Learning with Recoverable Forgetting](https://arxiv.org/abs/2207.08224) | 2022 | Ye et al.  | _ECCV_ | LIRF | - |  |\n","| [Continual Learning and Private Unlearning](https://arxiv.org/abs/2203.12817) | 2022 | Liu et al. | _CoLLAs_ | CLPU | [[Code]](https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning) | |\n","| [Verifiable and Provably Secure Machine Unlearning](https://arxiv.org/abs/2210.09126) | 2022 | Eisenhofer et al. | _arXiv_ | - | [[Code]](https://github.com/cleverhans-lab/verifiable-unlearning) |  Certified Removal Mechanisms |\n","| [VeriFi: Towards Verifiable Federated Unlearning](https://arxiv.org/abs/2205.12709) | 2022 | Gao et al. | _arXiv_ | VERIFI | - | Certified Removal Mechanisms |\n","| [FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information](https://arxiv.org/abs/2210.10936) | 2022 | Cao et al. | _S&P_ | FedRecover | - | recovery method |\n","| [Fast Yet Effective Machine Unlearning](https://arxiv.org/abs/2111.08947) | 2022 | Tarun et al. | _arXiv_ | UNSIR | - |  |\n","| [Membership Inference via Backdooring](https://arxiv.org/abs/2206.04823) | 2022 | Hu et al.  | _IJCAI_ | MIB | [[Code]](https://github.com/HongshengHu/membership-inference-via-backdooring) | Membership Inferencing |\n","| [Forget Unlearning: Towards True Data-Deletion in Machine Learning](https://arxiv.org/abs/2210.08911) | 2022 | Chourasia et al. | _ICLR_ | - | - | noisy gradient descent |\n","| [Zero-Shot Machine Unlearning](https://arxiv.org/abs/2201.05629) | 2022 | Chundawat et al. | _arXiv_ | - | - |  |\n","| [Efficient Attribute Unlearning: Towards Selective Removal of Input Attributes from Feature Representations](https://arxiv.org/abs/2202.13295) | 2022 | Guo et al. | _arXiv_ | attribute unlearning | - |  |\n","| [Few-Shot Unlearning](https://download.huan-zhang.com/events/srml2022/accepted/yoon22fewshot.pdf) | 2022 | Yoon et al.   | _ICLR_ | - | - |  |\n","| [Federated Unlearning: How to Efficiently Erase a Client in FL?](https://arxiv.org/abs/2207.05521) | 2022 | Halimi et al. | _UpML Workshop_ | - | - | federated learning |\n","| [Machine Unlearning Method Based On Projection Residual](https://arxiv.org/abs/2209.15276) | 2022 | Cao et al. | _DSAA_ | - | - |  Projection Residual Method |\n","| [Hard to Forget: Poisoning Attacks on Certified Machine Unlearning](https://ojs.aaai.org/index.php/AAAI/article/view/20736) | 2022 | Marchant et al. | _AAAI_ | - | [[Code]](https://github.com/ngmarchant/attack-unlearning) | Certified Removal Mechanisms |\n","| [Athena: Probabilistic Verification of Machine Unlearning](https://web.archive.org/web/20220721061150id_/https://petsymposium.org/popets/2022/popets-2022-0072.pdf) | 2022 | Sommer et al. | _PoPETs_ | ATHENA | - | |\n","| [FP2-MIA: A Membership Inference Attack Free of Posterior Probability in Machine Unlearning](https://link.springer.com/chapter/10.1007/978-3-031-20917-8_12) | 2022 | Lu et al. | _ProvSec_ | FP2-MIA | - | inference attack |\n","| [Deletion Inference, Reconstruction, and Compliance in Machine (Un)Learning](https://arxiv.org/abs/2202.03460) | 2022 | Gao et al. | _PETS_ | - | - |  |\n","| [Prompt Certified Machine Unlearning with Randomized Gradient Smoothing and Quantization](https://openreview.net/pdf?id=ue4gP8ZKiWb) | 2022 | Zhang et al.   | _NeurIPS_ | PCMU | - | Certified Removal Mechanisms |\n","| [The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining](https://arxiv.org/abs/2203.07320) | 2022 | Liu et al. | _INFOCOM_ | - | [[Code]](https://github.com/yiliucs/federated-unlearning) |  |\n","| [Backdoor Defense with Machine Unlearning](https://arxiv.org/abs/2201.09538) | 2022 | Liu et al. | _INFOCOM_ | BAERASER | - | Backdoor defense |\n","| [Markov Chain Monte Carlo-Based Machine Unlearning: Unlearning What Needs to be Forgotten](https://dl.acm.org/doi/abs/10.1145/3488932.3517406) | 2022 | Nguyen et al. | _ASIA CCS_ | MCU | - | MCMC Unlearning  |\n","| [Federated Unlearning for On-Device Recommendation](https://arxiv.org/abs/2210.10958) | 2022 | Yuan et al. | _arXiv_ | - | - |  |\n","| [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher](https://arxiv.org/abs/2205.08096) | 2022 | Chundawat et al. | _arXiv_ | - | - | Knowledge Adaptation |\n","| [ Efficient Two-Stage Model Retraining for Machine Unlearning](https://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kim_Efficient_Two-Stage_Model_Retraining_for_Machine_Unlearning_CVPRW_2022_paper.html) | 2022 | Kim and Woo | _CVPR Workshop_ | - | - |  |\n","| [Learn to Forget: Machine Unlearning Via Neuron Masking](https://ieeexplore.ieee.org/abstract/document/9844865?casa_token=_eowH3BTt1sAAAAA:X0uCpLxOwcFRNJHoo3AtA0ay4t075_cSptgTMznsjusnvgySq-rJe8GC285YhWG4Q0fUmP9Sodw0) | 2021 | Ma et al. | _IEEE_ | Forsaken | - | Mask Gradients |\n","| [Adaptive Machine Unlearning](https://proceedings.neurips.cc/paper/2021/hash/87f7ee4fdb57bdfd52179947211b7ebb-Abstract.html) | 2021 | Gupta et al. | _NeurIPS_ | - | [[Code]](https://github.com/ChrisWaites/adaptive-machine-unlearning) | Differential Privacy |\n","| [Descent-to-Delete: Gradient-Based Methods for Machine Unlearning](https://proceedings.mlr.press/v132/neel21a.html) | 2021 | Neel et al. | _ALT_ | - | - | Certified Removal Mechanisms |\n","| [Remember What You Want to Forget: Algorithms for Machine Unlearning](https://arxiv.org/abs/2103.03279) | 2021 | Sekhari et al. | _NeurIPS_ | - | - |  |\n","| [FedEraser: Enabling Efficient Client-Level Data Removal from Federated Learning Models](https://ieeexplore.ieee.org/abstract/document/9521274) | 2021 | Liu et al. | _IWQoS_ | FedEraser | - |  |\n","| [Federated Unlearning](https://arxiv.org/abs/2012.13891) | 2021 | Liu et al. | _IWQoS_ | FedEraser | [[Code]](https://www.dropbox.com/s/1lhx962axovbbom/FedEraser-Code.zip?dl=0) |  |\n","| [Machine Unlearning via Algorithmic Stability](https://proceedings.mlr.press/v134/ullah21a.html) | 2021 | Ullah et al. | _COLT_ | TV | - | Certified Removal Mechanisms |\n","| [EMA: Auditing Data Removal from Trained Models](https://link.springer.com/chapter/10.1007/978-3-030-87240-3_76) | 2021 | Huang et al. | _MICCAI_ | EMA | [[Code]](https://github.com/Hazelsuko07/EMA) | Certified Removal Mechanisms |\n","| [Knowledge-Adaptation Priors](https://proceedings.neurips.cc/paper/2021/hash/a4380923dd651c195b1631af7c829187-Abstract.html) | 2021 | Khan and Swaroop | _NeurIPS_ | K-prior | [[Code]](https://github.com/team-approx-bayes/kpriors) | Knowledge Adaptation |\n","| [PrIU: A Provenance-Based Approach for Incrementally Updating Regression Models](https://dl.acm.org/doi/abs/10.1145/3318464.3380571) | 2020 | Wu et al. | _NeurIPS_ | PrIU | - | Knowledge Adaptation |\n","| [Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks](https://arxiv.org/abs/1911.04933) | 2020 | Golatkar et al. | _CVPR_ | - | - | Certified Removal Mechanisms |\n","| [Learn to Forget: User-Level Memorization Elimination in Federated Learning](https://www.researchgate.net/profile/Ximeng-Liu-5/publication/340134612_Learn_to_Forget_User-Level_Memorization_Elimination_in_Federated_Learning/links/5e849e64a6fdcca789e5f955/Learn-to-Forget-User-Level-Memorization-Elimination-in-Federated-Learning.pdf) | 2020 | Liu et al. | _arXiv_ | Forsaken | - |  |\n","| [Certified Data Removal from Machine Learning Models](https://proceedings.mlr.press/v119/guo20c.html) | 2020 | Guo et al. | _ICML_ | - | - | Certified Removal Mechanisms |\n","| [Class Clown: Data Redaction in Machine Unlearning at Enterprise Scale](https://arxiv.org/abs/2012.04699) | 2020 | Felps et al. | _arXiv_ | - | - | Decremental Learning |\n","| [A Novel Online Incremental and Decremental Learning Algorithm Based on Variable Support Vector Machine](https://link.springer.com/article/10.1007/s10586-018-1772-4) | 2019 | Chen et al. | _Cluster Computing_ | - | - | Decremental Learning  |\n","| [Making AI Forget You: Data Deletion in Machine Learning](https://papers.nips.cc/paper/2019/hash/cb79f8fa58b91d3af6c9c991f63962d3-Abstract.html) | 2019 | Ginart et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","| [Lifelong Anomaly Detection Through Unlearning](https://dl.acm.org/doi/abs/10.1145/3319535.3363226) | 2019 | Du et al. | _CCS_ | - | - |  |\n","| [Learning Not to Learn: Training Deep Neural Networks With Biased Data](https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Learning_Not_to_Learn_Training_Deep_Neural_Networks_With_Biased_CVPR_2019_paper.html) | 2019 | Kim et al. | _CVPR_ | - | - |  |\n","| [Efficient Repair of Polluted Machine Learning Systems via Causal Unlearning](https://dl.acm.org/citation.cfm?id=3196517) | 2018 | Cao et al. | _ASIACCS_ | KARMA | [[Code]](https://github.com/CausalUnlearning/KARMA) |  |\n","| [Understanding Black-box Predictions via Influence Functions](https://proceedings.mlr.press/v70/koh17a.html) | 2017 | Koh et al. | _ICML_ | - | [[Code]](https://github.com/kohpangwei/influence-release) | Certified Removal Mechanisms |\n","| [Towards Making Systems Forget with Machine Unlearning](https://ieeexplore.ieee.org/abstract/document/7163042) | 2015 | Cao and Yang | _S&P_ | - |  |\n","| [Towards Making Systems Forget with Machine Unlearning](https://dl.acm.org/doi/10.1109/SP.2015.35) | 2015 | Cao et al. | _S&P_ | - | - | Statistical Query Learning  |\n","| [Incremental and decremental training for linear classification](https://dl.acm.org/doi/10.1145/2623330.2623661) | 2014 | Tsai et al. | _KDD_ | - | [[Code]](https://www.csie.ntu.edu.tw/~cjlin/papers/ws/) | Decremental Learning  |\n","| [Multiple Incremental Decremental Learning of Support Vector Machines](https://dl.acm.org/doi/10.5555/2984093.2984196) | 2009 | Karasuyama et al. | _NIPS_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Learning for Linear Support Vector Machines](https://dl.acm.org/doi/10.5555/1776814.1776838) | 2007 | Romero et al. | _ICANN_ | - | - | Decremental Learning  |\n","| [Decremental Learning Algorithms for Nonlinear Langrangian and Least Squares Support Vector Machines](https://www.semanticscholar.org/paper/Decremental-Learning-Algorithms-for-Nonlinear-and-Duan-Li/312c677f0882d0dfd60bfd77346588f52aefd10f) | 2007 | Duan et al. | _OSB_ | - | - | Decremental Learning  |\n","| [Multicategory Incremental Proximal Support Vector Classifiers](https://link.springer.com/chapter/10.1007/978-3-540-45224-9_54) | 2003 | Tveit et al. | _KES_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Proximal Support Vector Classification using Decay Coefficients](https://link.springer.com/chapter/10.1007/978-3-540-45228-7_42) | 2003 | Tveit et al. | _DaWak_ | - | - | Decremental Learning  |\n","| [Incremental and Decremental Support Vector Machine Learning](https://dl.acm.org/doi/10.5555/3008751.3008808) | 2000 | Cauwenberg et al. | _NeurIPS_ | - | - | Decremental Learning  |\n","----------"]},{"cell_type":"markdown","id":"937e8ebf","metadata":{"papermill":{"duration":0.005298,"end_time":"2023-07-05T23:39:33.409793","exception":false,"start_time":"2023-07-05T23:39:33.404495","status":"completed"},"tags":[]},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":12.613014,"end_time":"2023-07-05T23:39:34.646883","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-05T23:39:22.033869","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}